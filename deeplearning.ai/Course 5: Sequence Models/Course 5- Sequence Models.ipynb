{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "### Why Sequence Models\n",
    "Used for:\n",
    "1. Speech Recognition\n",
    "2. Music Generation\n",
    "3. Sentiment Classification\n",
    "4. DNA sequence analysis\n",
    "5. Machine translation\n",
    "6. Video Activity recognition\n",
    "7. Name entity recognition\n",
    "\n",
    "### Notation\n",
    "$$x^{n}$$\n",
    "Used to denote the elements of an input sequence, $t^{<i>}$ for the length of the sequence. \n",
    "\n",
    "To represent a word create a vocabulary where each possible word ha s an index. We can use a one-hot encoding for each word in our sentence. \n",
    "\n",
    "### Recurrent Neural Network\n",
    "Recurrent Neurla networks have a common visualization with several tall matricies that represent the layers/vocabularies. This is called the unrolled RNN diagram. Sometimes there is a version of the diagram that shows a single layer with a looping arrow. \n",
    "\n",
    "Tanh are often used for rnn, though sometimes relu. $W_{ya}$ is used to denote a weight matrix for the output quantity. \n",
    "\n",
    "### Backpropagation Through Time\n",
    "Loss function: standard logistic regression lost, cross-entropy loss.\n",
    "\n",
    "Backprop for RNN is called backprop through time.\n",
    "\n",
    "### Different Types of RNNs\n",
    "1. Many-to-one \n",
    "2. Many-to-many\n",
    "3. One to Many\n",
    "4. One to One\n",
    "5. Many to Many (w/ encoder and decoder)\n",
    "\n",
    "You can use an architecture that takes a single input and outputs a several y values in a one-to-many architecture. In the diagram you also oftne have connections between the output y hat and the next neuron. \n",
    "\n",
    "Another many-to-many architecture example is machine translation where there is an encoder where x values are input and a decoder where ys are output. \n",
    "\n",
    "### Language Model and Sequence Generation\n",
    "A speech recognition example where a probabaility that input is a valid sentence is output. In this many to one example we also need to toneknize our input sentences and added and Eos, end of sentence exmaple and a token for words that are not a part of the corpus. \n",
    "\n",
    "### Sample Novel Sequences\n",
    "Sample from the first layer in the sequence and pass that to the next neuron in your sequence. Take that output and pass it to the next neuron and on... continue unitl you hit the EOS token. \n",
    "\n",
    "### Vanishing gradients with RNNs\n",
    "Language can have long term dependencies. In english you can have a subject that is referenced later in the sentence and the rnn needs to track singular/plural over the arbitrarily long middle section- this can be very difficult to keep track of, and for the error to backprop through the RNN. **Exploding and vanishing gradients is a much larger and more common issue in RNNs**. Exploding gradient s are easy to spot because you'll start seeing NaNs. A way to fix this is to introduce **gradient clipping**.\n",
    "\n",
    "### Gated Recurrent Unit\n",
    "RNN acitvation formula\n",
    "$$a^{<t>}=g(W_a [a^{<t-1>}, x^{<t>}]+b_a)$$\n",
    "Where g is a tanH function.\n",
    "\n",
    "The GRU is like the RNN but it has a memory cell, $c^{<t>}$ here c and a are the same value but denoted differently because semantically one is the meomory cell value, and the other is the output activation value, and this difference will become important in LSTMs. \n",
    "\n",
    "Equations of GRU:\n",
    "$$\\tilde{c}^{<t>} = tanh(w_c[c^{<t-1>}, x^{<t>}] + b_c)$$\n",
    "Update Gate, between 0 and 1:\n",
    "$$\\Gamma_{u}= \\sigma(w_c [c^{<t-1>}, x^{<t>}] + b_u)$$\n",
    "\n",
    "The key part of the GRU is that we have some memory cell, C, and an update gate that decides if we update it or not. The whole equation for the gru is the memory cell C = the gate times the candidate memory vlaue plus the 1 minus the gate times the old value.\n",
    "\n",
    "$$c^{<t>}= \\Gamma_{u}*\\tilde{c}^{<t>}+(1-\\Gamma_{u})*c^{<t-1>}$$\n",
    "\n",
    "If you remember the images from Colah's blog we can understnad how the gru works. It takes the output from the previous cell and uses that to compute the candidate and the gate, Gamma. It uses the output of these two operations to construct the last formula where we output the memory value of the cell. The last equation can be denoted as $\\Gamma_u$ and $\\Gamma_r$ meaning the update gate and relevance gate\n",
    "\n",
    "### Long Short-Term Memory Unit\n",
    "Equations for the LSTM:\n",
    "$$\\tilde{c}^{<t>} = tanh(w_c[c^{<t-1>}, x^{<t>}] + b_c)$$\n",
    "\n",
    "Gamma Update Gate-\n",
    "$$\\Gamma_{u}= \\sigma(w_u [c^{<t-1>}, x^{<t>}] + b_u)$$\n",
    "\n",
    "Gamma Forget Gate-\n",
    "$$\\Gamma_{f}= \\sigma(w_f [c^{<t-1>}, x^{<t>}] + b_f)$$\n",
    "\n",
    "Gamma Update Gate-\n",
    "$$\\Gamma_{o}= \\sigma(w_o [c^{<t-1>}, x^{<t>}] + b_o)$$\n",
    "\n",
    "Final equation-\n",
    "$$c^{<t>}= \\Gamma_{u}*\\tilde{c}^{<t>}+(\\Gamma_{f})*c^{<t-1>}$$\n",
    "\n",
    "It has the option of keeping the old value and just adding to it rather than keep or throw out.\n",
    "\n",
    "Sometimes they also have a 'peephole' connection where c from the previous cell get passed directly to the LSTM. \n",
    "\n",
    "### Bidirectional RNN\n",
    "Computes forward and backward to understand sequence dependencies in both directions. \n",
    "\n",
    "### Deep RNNs\n",
    "An RNN with several layered sequences of neurons. Because of the temporal dimension 3 layers is pretty deep for neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "### Word Representation\n",
    "Words in the vocabulary are represented as a one-hot encodeing\n",
    "\n",
    "These word representations have no sense of similarity of words, for that we use **featurized representation word embeddings** where each word in the dictionary has a value of association with the other words in the dictionary. \n",
    "\n",
    "Visualizing word embeddings: t-sne, t stochastic neiborhood embeddings\n",
    "\n",
    "Once you have trained a model that uses word embeddings as a model you can then use those instead of the one-hot encoding as word representations in your RNN. You can use the word embeddings to do transfer learning by trainging a large corpus to create word-embeddings and then transfer those embeddings to a new task with a smaller training set\n",
    "\n",
    "### Properties of word embeddings\n",
    "Can help with analogic reasoning. We can use the word embeddings in simple mathematical expression like $e_{man} - e_{woman} \\approx e_{king} - e_w$\n",
    "\n",
    "You can use cosine similarity to compare and look at the similarity of two different word embeddings. These analogies can pick up on similar capitals, generdered words, etc. \n",
    "\n",
    "### Learning Embedding Matrix\n",
    "We use a specialized function for looking up the embedding because we cannot efficiently do the mulitplication over the embedding matrix. \n",
    "\n",
    "There are many attempts to learn word embeddings:\n",
    "1. **Skipgram:** Use context/target pairs by looking a set of adjacent words to understand the context/ menaing of the word. The downside is that the softmax objective is slow to learn\n",
    "2. **Neural Translation:**\n",
    "3. **Word2Vec:** \n",
    "4. **Negative Sampling:** Similar to the skipgram model but simpler. It computes 10,000 binary classification problems rather than 10,000 weight softmax classifier like skipgram. \n",
    "5. **Glove:** Not as popular as skipgram or word2vec, but it is very simple algorithm. Let x_ij be the times tha ti appears in context of j. \n",
    "\n",
    "### Application \n",
    "#### Sentiment Classification \n",
    "1. Average the embeddings for each of the words in the comment\n",
    "2. Get the embedding vectors for each of the words of the comment and feed them into an rnn. \n",
    "3. Debiased word embeddings- Addressing gender stereotypes in algorithms. Take the differences between gender embeddings and average them- use this to identify the bias direction. Neutralize, for every word that is not gendered by definition, project them onto the bias axis and remove the bias component/direction. Equalize pairs, when we have gendered pairs where the words are definitionally gendered the distance should be the same compared to words that are gender neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "### Basic Models \n",
    "Sequence to Sequence Model:\n",
    "Uses an encoding and decoding netowrk and can be used for language ot language translation. Another example is image captioning. A conditoinal language model is used for translation conditioned on another language. Here we do not want ot sample from the probabilities of translation, we want to maximiaze the propability using an arg max function on the probability. In a **groeedy search** you would get the best answer at each step 1 and after. In a **beam search** you maximize the best conditional probability. \n",
    "\n",
    "### Beam Search\n",
    "Beam search will first look at the first parts of the english sentence. Beam search has a hyperparameter Beam width, which is how many things it considers at once. It then looks for the pair of the first and second words that are most likely.  At each step in the process beam search instantiates three versions of the network. \n",
    "\n",
    "### Refinements to Beam Search\n",
    "1. Length normalization- When computing the product summation of a many small probabilities we instead use the sum of the log. Longer sentence are unlikely to be picked, so we can normalize this with 1/ty^alpha to reduce the penalty to long sentences. \n",
    "2. Large B: Considers many options but it is slower, a smll beam width is faster with a worst result. There will be huge gains when going from 1 t0 10, or to 100, but you will get diminishing returns when going to 1000 or 3000. \n",
    "\n",
    "### Error Analysis for Beam Search\n",
    "Compare P-hat the decision by the algorithm and p* the decision by a human. If the y*, human decision, has a larger p(y|x) values then beam search made a mistake. \n",
    "\n",
    "When doing error analysis of our RNN we oten want to figure out what is at fault, the RNN or the Beam search algorithm. We can make a table and work throught which is at fault. \n",
    "\n",
    "### Bleu Score\n",
    "Used to evaluate machine translation. \n",
    "\n",
    "### Attention Model Intuition \n",
    "Earlier we looked at encoder decoder models but these do not function the way a real human translator would- instead they would look at a little bit of the sentence and translate it, then look at the next few words in the sentence and translate it. The problem with this is the limitaitons of long sentences/sequences. \n",
    "\n",
    "An attention model uses a value, $\\alpha^{<1, 1>}$ to determine how much should it be paying attention to this first value. \n",
    "\n",
    "### Attention Model \n",
    "We have a state that produces y(1) and is given context c. Context, c, is a function that takes alpha^(1, 0), alpha(1, 1) and all the alphas from the words.  \n",
    "\n",
    "$$a^{<t, t'>} = amount of attention y^{<t>} should pay to \\alpha^{t'}$$\n",
    "\n",
    "$$a^{<t,t'>} = \\frac{exp(e^{<t,t'>})}{\\Sigma^{T_x} exp(e^{<t, t'>})}$$\n",
    "\n",
    "A great way of understanding the attention model is to plot the rates of attention against the words in a chart and you can see which words garnered more attention than there expected positon.\n",
    "You see where it paying attention and when. \n",
    "\n",
    "### Speech Recognition \n",
    "An audio clip is measuring changes in air pressure, changes in air pressure over time. You can take the audio clip and transform it into a spectragram, where the values can be represented as an images. \n",
    "\n",
    "Speech recognition used to run on hand-spun models using phonemes. \n",
    "\n",
    "CTC, **Connectionist temporal classification, is used as a cost funtion for speech recognition**.\n",
    "\n",
    "### Trigger Word Detection\n",
    "Trigger word detection is used in Alexa, Google Home, etc with a single word. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
