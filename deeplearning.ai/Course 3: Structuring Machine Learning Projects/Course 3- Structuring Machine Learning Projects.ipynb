{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "### Error Analysis\n",
    "1. When finging that some examples are often wrongly classified manually look at examples and see where the classifier fails, then count up and determine a percentage of cases where the classifier is wrong. \n",
    "2. You can break out the errors based on category, looking systematically at where the failures in you model come from. This will help you prirotize where your failures are coming from, especially in situations where you might have data that is incorrectly labeled. When you create categories for why errors in the classifier have happened- add a category for incorrectly labeled data. \n",
    "3. Aggregate the percentages of sources of you errors and use that to decide what in your model should be focused on. \n",
    "\n",
    "### Build your first system quickly, then iterate\n",
    "Taking speech recognition for example, there are many little problems and edge cases that you can work on to increase the accuracy of your model, getting a model created and testing it with  people will help you find what errors are most important to handle first and helps prioritize your next steps. Teams usually overhtink problems rahter than underthink so you should work quick and dirty. \n",
    "\n",
    "### Training and testing on different distributions\n",
    "ML algorithms are hungary for data and teams are increasingly finding themselves training on data that has a different distribution than the test and dev set. There are best practices when training and testing from two different distributions. \n",
    "\n",
    "**Use most of the target distribution for the dev/test set and use the \"wrong\" distribution for the train with a bi of the smaller target distribution**\n",
    "\n",
    "    -20k target examples\n",
    "    -500k examples from other distribution\n",
    "    \n",
    "    -510k test with 10k from target\n",
    "    -5k dev from target distribution\n",
    "    -5k test from target distribution\n",
    "\n",
    "### Transfer Learning\n",
    "Transfer learning is where you train on one task and attempt to transfer the learning from that task to another task. \n",
    "\n",
    "### Multi-task learning\n",
    "Unlike transfer learning where you train on some task and then retrain to another task in multi-task learning you are trying to simaltaneously train on several different tasks. Multi-ask learning can be learning to distiguish many features from an image, like seeing cars, people, traffic lights, and buildings in a driving image dataset. Multi-task learning makes the most sense when the set of tasks benefit from having shared lower-level features. Multi-task learning is rare- computer vision is where it is most common. \n",
    "\n",
    "### End-to-end deep learning\n",
    "Deep learning that used to be broken into multiple stages of processing with many stages can be replaced with deep learning models that replace the many steps from the pipeline. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
