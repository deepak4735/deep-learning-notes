{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "### Train/ Test/ Dev\n",
    "ML is highly iterative, you have to choose many factors:\n",
    "1. Layers\n",
    "2. Hidden Units\n",
    "3. Learning Rates\n",
    "4. Activation Functions\n",
    "\n",
    "It is an often iterative process because you rarely know what is going to work. Take an idea- code- and experiment. \n",
    "\n",
    "ML has worked in NLP, Vision, Speech, Structural Date like Ads, Search, and Security and Logistics. \n",
    "\n",
    "Setting up your train/test and development are key to being more efficient. \n",
    "\n",
    "Break up your train set, validation/dev set, and then lastly the test set. \n",
    "\n",
    "Use your training set to break up your data and test it on the dev set and then after having done this long enough- use your test set last. For an unbiased evaluation \n",
    "\n",
    "Traditional : 60%/ 20%/ 20% (maybe 100/1000/10000 examples)\n",
    "Big Data: 98% / 1% / 1% (1,000,000 examples)\n",
    "\n",
    "Dev sets have started getting smaller because you just need enough to get a sense of wich of your models are better. Maybe you only need 10,000 for your training and 10,000 for your test set. The traditional divide can work fine if you are working with a small test set. \n",
    "\n",
    "Many people train on mismatched train/test distributions. This happens because the need to get lots of data and can end with disparities in performance if it trained on categorically different data than the test- to fix this make sure you train and test come from the same distribution. \n",
    "\n",
    "It might be okay to not have a test set- only the dev set. \n",
    "Some time people will use the dev set as a 'test' set and it becomes more like a hold out cross validation set. \n",
    "\n",
    "### Bias / Variance\n",
    "\n",
    "Bias and Variance take a long time to master. There used to be a conversation about the bias variance tradeoff but with big data it is less of a tradeoff. \n",
    "\n",
    "Comparing training set to dev set error if we have high dev set error but low training set error it is a sign of high variance or overfitting, if we have high error for both training set and test set it is a sign of high bias or underfitting. \n",
    "\n",
    "**High bias**- look at the training set performance. Try a bigger network or more layers or more units. Train it longer or try more advanced optimization algorithms (sometimes there are better NN architectures) \n",
    "\n",
    "**High Variance**- More data, regularization, regularization, better NN architecture. \n",
    "\n",
    "Bias Variance Tradeoff- An old conversation pre-deep learnng era there weren't many tools that just reduced bias or just reduced variance without hurting the other one. Getting more data should always reduce variance without hurting bias too much. \n",
    "\n",
    "### Regularization\n",
    "\n",
    "One of the things you can do to deal with too much variance that is not so costly is use regularizaiton. \n",
    "\n",
    "L2 regularization is the euclidean norm of the weight matrix. We then multiply it by the lambda divided by 2M. \n",
    "\n",
    "L1 regularization is lambda over M times the sum of the w vector- L1 regularization is said to make the matrix sparse- and this helps compress the model. Lambda is the regularizaiton parameter. \n",
    "\n",
    "In a Neural network- there is also the Frobenius norm\n",
    "Frobenius Norm - The sum of sqaured elements of a matrix, the same as the L2 norm of a matrix but it goes by a different name. \n",
    "\n",
    "How do you implement the back prop when regularizing the matrix. You can simple add the regularization term (lambda/ M ||W||) to the dj/dw (the derivative with respect to w). \n",
    "\n",
    "L2 Regularization is also called weight decay. \n",
    "\n",
    "### Why Does Regularization help with overfitting\n",
    "\n",
    "Regularizaiton basically simplifies the network by zeroing out/simplifying or getting rid of many of the hidden units, w. \n",
    "\n",
    "### Dropout Regularizaiton\n",
    "\n",
    "Everytime you flip a coin there is a 0.5 chance that remove a node, then you remove all the ingoing, and outgoing links as well. Then you perform backprop over this network. And repeat culling nodes at random. \n",
    "\n",
    "Dropout can be performed several ways:\n",
    "\n",
    "**Inverted dropout**:\n",
    "Create a vector the size of the layer inquestion, and set that layer to random's at the keep probability.Then multiply the layer vector by this new vector to shut off some of the neurons. Dont forget to account for these neurons by dividing the input to accord with the dropped out neurons. \n",
    "\n",
    "You can vary keep-prop by layer. The input layer rarely has a dropout- or atleast a very small one. This gives you another hyperparmeter to very in your training. \n",
    "\n",
    "Dropout is a regularizaiton technique used for over fitting, and while some people like computer vision researchers might use it as a default, it is for overfitting and you shouldn't really use it otherwise. Computer vision people use it becuase they almost always never have enough data and are over-fitting. \n",
    "\n",
    "### Other regularization techniques\n",
    "1. Flip image horizontally and add that to the training set, this is not as good but it is inexpensive. You can also randomly crop and transform the image. \n",
    "2. Early stopping. Plot the error on cost function J, the function you are training. Also plot the dev set error and see that the dev set error will go down and then increase at somepoint. Take whatever value achieved this dev set error. By stopping half-way you stop before you start overfitting on j. \n",
    "3. Orthogonalization, the following principle. Ng perfers to break machine learning down into two categories- one where we are optimizing for J(w, b) and another where we are making sure not to overfit. Early stopping breaks this principle, but early stopping is computationally more efficient compared to L2 regularization where you have to try several values of lambda. \n",
    "\n",
    "### Normalizing Inputs\n",
    "1. Subtract mean. 1/m sum of Xs\n",
    "2. Normalize the variances 1/m sum x**2\n",
    "\n",
    "You normalize the inputs to change the shape of your cost function, it can often look like an elongated bowl shape and if you normalize the features your cost function will on average look more symmetric andyou can use a larger learning rate than the elongated bowl. \n",
    "\n",
    "### Vanishing / Exploding Gradient\n",
    "\n",
    "Imagine you are training a very deep neural network. Due to the chain rule and the- blah blah. The way to deal with the vanishing gradient is to properly initialize the weights. \n",
    "\n",
    "### Weight Initialization for Deep Networks\n",
    "\n",
    "In order to make sure Z does not explode,\n",
    " \n",
    "w = np.random.randN(slope)*np.sqrt(2/n)\n",
    " \n",
    "If you use tanH activation function then use a Xavier initialization which is a different formula. \n",
    " \n",
    "Use this when traing deep networks to make your networks train much more quickly. \n",
    "\n",
    "Initialization types:\n",
    "1. Zero\n",
    "2. Random\n",
    "3. Xavier\n",
    "\n",
    "### Numerical approzimation of gradients\n",
    "\n",
    "You can get a better approximation of the gradients by taking the derivative over a larger portion of the curve with a triangle above and below. \n",
    "\n",
    "**Gradient Checking** is a great way to debug your neural networks. \n",
    "Reshape all the W and b vectors re-shape them into a single big vector- and rename theta. REshape gradient vectros dW, db, into dTheta. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "MIni-batch lets you start doing gradient descent without iterating through all of your training sets.\n",
    "\n",
    "Vecorization allows you to efficiently compute on m examples. \n",
    "However, with 1,000,000 examples you are still going to be very slow to finish your computation. So you break up the training sets into mini-batches, where mini batch t: {x, y} written with curly brackets. \n",
    "\n",
    "The x and y are written with a {t} superscript\n",
    "\n",
    "Batch gradient descent is the gradient descent we've been talking about, where we process our update on all the training examples all at once. \n",
    "\n",
    "Stochastic gradient descent- you process a single example at a time, rather than a batch of size M, like mini batch gradient descent. It can be very noise but you can deal with this by using a smaller learning rate. \n",
    "\n",
    "Using a mini-batch size of power of 2 gives you gains in processing but ultimately it is another hyperparameter\n",
    "\n",
    "\n",
    "### Exponetially Weighted Averages\n",
    "This is a technique to dampen the oscillations in our learning as we go towards the global minima. Sometimes as we have a high learning rate we can osciallate in the direction orthogonal to our goal, RMSProp deals with this. \n",
    "\n",
    "\n",
    "### Adam Optimiztion algorithim\n",
    "Adam is a combinaiton of momentum and RMSProp, it works well on a variety of probelms. \n",
    "\n",
    "### Tuning Process\n",
    "1. Search randomly ( not-gris)\n",
    "2. Implement coarse-to-fine process\n",
    "\n",
    "Hyperparams:\n",
    "1. alpha - learning term (most important)\n",
    "2. beta - momentum term (2 important)\n",
    "3. Beta1, Beta2, Epsilon- ADAM terms (always use .9, .999, 10^-8) \n",
    "4. # layers (3 important ) \n",
    "5. # Hidden units (2 important)\n",
    "6. Learning rate decay (3 important)\n",
    "7. Mini-batch size (2 important)\n",
    "\n",
    "Other formula: J cost function\n",
    "\n",
    "Grid-search used to be used often when finding hyper-paramters. This used to be okay when the amount of parameters was small. \n",
    "\n",
    "The suggestion in deep learning is to try out random hyperparameter values. The benifit of comparing grid search vs random is that in random we are giving ourselves the chance to try out 25 different values of a variable rather than 5. \n",
    "\n",
    "When we search within the space of our hyper paramters, especialy as we add more variables we want to search course to fine. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
