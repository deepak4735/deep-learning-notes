{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectures\n",
    "\n",
    "A neuron computes a linear function (z= Wx + b) followed by an activation function\n",
    "\n",
    "In machine learning we often normalize our data because normalized data converges faster. Like changing x to x / euclidean_norm(x) where the euclidean norm of every **row** is calculated. Then each row of x should be a vector of unit length, length of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguation: Dot Product, Cartesian Product, and Element Wise Multiplicatoin(Hadamard)\n",
    "\n",
    "The dot product, also the inner-product returns a single number by multiplying two same size arrays and adding the results. What matters in the dot product of two arrays (1555, 10) and (10, 85855) is that they both have the same amount of operations, the resulting matrix would be (1555, 85855) with 10 multiplication operations.\n",
    "\n",
    "The cartesian product is a set of all combinations from two other sets. This can be thought of as the construction of a m,n matrix of tuples where m and n are the lengths of the constiuent arrays.\n",
    "\n",
    "The element-wise of hadamrd product is the mutliplication of two-matrices, that will throw an error if the two matrices are not of proper lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1229, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(1229, 150)\n",
    "b = np.random.randn(150, 45)\n",
    "c = np.dot(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.52519334  1.73189109  0.70168702  0.3538837 ]\n",
      " [ 0.24775749  0.63814992 -0.50328269  0.36621863]\n",
      " [ 0.58791099 -0.20733617  1.11491813 -0.25542607]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "b = np.random.randn(4, 1)\n",
    "c = a + b.T\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sigmoid= 1 / (1 + np.exp(-x))\n",
    "    derivative_sigmoid = sigmoid(1 - sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps\n",
    "\n",
    "## Normalize Data Row\n",
    "Normalize the rows of your data by divididing each data point by the Euclidean norm of the entire row `||x||` which is sqrt(sum(pow(x, 2)))\n",
    "\n",
    "## Find data dimensions\n",
    "Find out the dimensions of the data, many problems in mahcine learning come from not knowing the dimensions of the data and making a mistake.\n",
    "\n",
    "## Flatten Test Data\n",
    "A common practice is to flatten test data so that every image/input is a single row. Numpy's flatten method is usedul for this. \n",
    "\n",
    "## Center and standardize\n",
    "Subtract the mean of the whole numpy array from each example then divide each example by the standard deviation of the whole numpy array. For image datasets it is simpler and more convienent and works almost as well to just divide every row of the dataset by 255. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model\n",
    "\n",
    "1. Calculate current loss (forward propogation)\n",
    "2. Calculate current gradient (backward propogaiton)\n",
    "3. Update parameters (gradient descent)\n",
    "\n",
    "<img src='https://github.com/Robotikus/deeplearning.ai/blob/master/assets/logistic_net.png?raw=true'>\n",
    "\n",
    "1. Initialize a vector of zeroes the size of the weights for the net\n",
    "2. Initialize a scalar that corresponds to the bias\n",
    "\n",
    "### Forward Propogate\n",
    "Computes the cost function and its gradients.\n",
    "1. Get X\n",
    "2. Compute weight times x plus bias for each layer\n",
    "3. the weights.T times x plus the bias gets passed to the activation function\n",
    "4. Calculate the cost function\n",
    "\n",
    "Compute the loss and the current gradient\n",
    "1. Cost function is calculated\n",
    "2. Calculate the derivatives with repspect to weights and with respect to the bias\n",
    "3. The new weight is equal to the weight - the learning rate times the derivative respect to the weight\n",
    "4. The new bias is computed the same way, by subtracting from the old bias by the learning rate times the derivative of the bias\n",
    "\n",
    "Prediction \n",
    "1. Prediction is simply repeating the same activation function but with the test x data and passing to a prediction layer that I beleive works as a sort of one-hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.48882126  0.87448832  1.84017082]\n",
      " [-0.39769527  0.20024956  2.15709579]\n",
      " [ 0.57743944 -0.93854064  1.65560288]\n",
      " [-0.22412324 -1.72256233  1.0541517 ]]\n",
      "[[ 1.22583789]\n",
      " [ 1.95965008]\n",
      " [ 1.29450169]\n",
      " [-0.89253386]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(4, 3)\n",
    "b = np.sum(a, axis= 1, keepdims= True)\n",
    "print(a)\n",
    "print(b)\n",
    "b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Week 3\n",
    "# Neural Network Representations\n",
    "\n",
    "In neural networks we do not count the input layer, also the output layer- usually y-hat can be labeled a with square brackets for the last layer in the neural network.  \n",
    "\n",
    "The convention is a superscript [Greek L] for the layer and subscript i for node in the layer. Both are based 1. Each neuron is broken into two parts, the first is z = the weight transposed times the x plus the bias. And the next is a, the activation equal to the sigmoid of z at that layer.  \n",
    "\n",
    "Remember to write subsequent layers in terms of the prevoius layers. \n",
    "\n",
    "The superscript in the parehteses to m, mean you are scannig through the training examples. Scanning left to right moves you through different training examples. Scanning up and down moves you through different layers in the network. \n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "The tanH function stands for the hyperbolic tangent and passes through 0, it is strictly always superior to the sigmoid/logistic function except in the output layer where if you would want the output to be between 0 and 1 because you are using binary classification. One problem with the tanH function is that if z, the input to a, is very large or very small the slope of the tanH function is close to zero and this can slow down gradient descent. \n",
    "\n",
    "Because of this the RELU or rectified linear unit is very popular. It has a linear line after 0. \n",
    "\n",
    "Relu is max(0, z)\n",
    "Leaky relu (0.01z, z) and the 0.01 can be another paramter in the neural net. \n",
    "\n",
    "#### How to choose an activation\n",
    "\n",
    "If your output is binary classification then use sigmoid function but otherwise the RELu function is increasingly the choice of activation function. There is another function the leaky relu, which usually works better than the relu function but it is not used as much in practice. \n",
    "\n",
    "\n",
    "### Why do you need an activation function? \n",
    "\n",
    " rms to zero is ok but intializing paramters to zero will cause a problem. \n",
    "\n",
    "When neuron weights are intialized the same then the entire first layer is computing the exact same function. All the hidden units are symmetric and it doesn't matter how long you run gradient descent they will all continue to run the same function. The solution is to intialize the paramters randomly. \n",
    "\n",
    "Biases, b, does not have this problem- named the symmetry breaking problem. \n",
    "\n",
    "When we initialize the weights we like to intialize them to very small values like 0.01, in tanH if we had a large value it would produce something near 0 when we pass the value z to the activation function g. \n",
    "\n",
    "np.random.randn(dims) * 0.01\n",
    "\n",
    "\n",
    "One more time:\n",
    "1. Choose the input neurons and the hidden layer\n",
    "2. Initialize the parameters and the biases\n",
    "3. Set loop for \n",
    "    1. Implement the forward propogation\n",
    "    2. Calculate the loos value\n",
    "    3. Implemnet the back propogation/ gradient\n",
    "    4. Use learning rate and gradient to update params (gradient descent)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Week 4\n",
    "\n",
    "Logistc regression is a shallow model. \n",
    "\n",
    "Capital L is the number of layers in a network.\n",
    "N superscript L is the number ofunits in layer l\n",
    "A superscript is the activations for layer l\n",
    "A superscript 0 is the input first layer \n",
    "A superscript captial L is the output last layer Y-hat\n",
    " \n",
    "**Andrew Ng** Tip for de-bugging \n",
    "Get a pen and paper and work through the dimensions of the matrix that I'm working on.\n",
    "\n",
    "### Why do networks need deep representations\n",
    "\n",
    "You can think about the layers of a neural network as first finding simple patterns and then composing them into more complex functions. Edge detectors are looking in very small areas of a picture and facial detectors are looking at much larger areas of the image. Begin with edges and then composing them to find even more complex things. \n",
    "\n",
    "In audio, the first level might look for low level audio waveform features. By composing low level features you may find phonemes, then words, and then sentences. \n",
    "\n",
    "### Circuit Theory and deep learning\n",
    "\n",
    "There is a connection between circuits like not gates and nor gates and deep learning, where to learn deeper features you need more layers like how in circuits for more complex tasks you need more types of gates. \n",
    "\n",
    "Paramters are how you pass around data in the forward propogation and, and cache is how you pass around data in the backwards propogation. \n",
    " \n",
    "### Parameters vs Hyperparameters\n",
    "\n",
    "1. Learning rate\n",
    "2. Hidden layers L\n",
    "3. Hidden units in n1, n2\n",
    "4. Choice of activation function \n",
    "5. Minibatch size\n",
    "6. Momentum (has to do with learning rate)\n",
    "7. Regularization terms\n",
    "\n",
    "Andrew Ng's advice is that often as people in ML move between disciplines their knowledge of values for hyperparameters carries over and sometimes not. So you should just try out a range of values.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
