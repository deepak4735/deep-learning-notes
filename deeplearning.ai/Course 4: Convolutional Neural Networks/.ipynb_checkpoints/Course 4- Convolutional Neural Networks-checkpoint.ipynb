{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "## Edge Detection\n",
    "\n",
    "If inpute image is 6x6 images, with a 3x3 filter will convolve to a 4x4 images. We use the element wise mulitplication where an asterisk is used in mathematics to denote the convolution. We multiply element-wise and add together the porducts to get the new value for the resulting 4x4 layer. \n",
    "\n",
    "### Vertical Edge Detection \n",
    "A filter for vertical edge detection could be a 3x3 filter with vertical rows of `[1 0 -1]` that when multiplied by a vertical edge will give you large values along the edge and zeroes elsewhere. Now the large values exist whereever there where edges in the previous image. The edge detection will pick up on the differences between light to dark and dark to light edges. \n",
    "\n",
    "### Horizontal Edge Detection \n",
    "\n",
    "`[[1,1,1], [0,0,0],[-1,-1,-1]]`\n",
    "\n",
    "Bright pixels on top and lihgt on the bottom will lead to positive values.\n",
    "\n",
    "There are many different values for filters dependig on what you want ot detect. For example, the Sobel filter: \n",
    "`[[1, 0, -1], [2, 0, -2], [1, 0, -1]]`\n",
    "Or a shaw filter- that also has it's own filters. \n",
    "\n",
    "Some researchers like to treat the values of these filters as a hyperparameter to learn. \n",
    "\n",
    "### Padding\n",
    "**Finding the size of the conv output**: \n",
    "NxN input, fxf filter, `[n-f+1 x n-f+1]`\n",
    "\n",
    "If everytime you convolve your image shrinks you will loose information if the result is too small, also values in the center of the matrix will be important in many operations while informaiton around the edge of the image gets thrown out. To fix these problems we can use padding. \n",
    "\n",
    "1. Shrinking outpSize of inpute units is total pixels in image, transformed into a single vertical array. out\n",
    "2. Throw away information around the edges\n",
    "\n",
    "Padding is adding an extra layer of zeroes around the input edges. The new result is now the same size as the input. When convolving there are two options, valid and same convolutions. \n",
    "\"valid\" is a no padding convolution \n",
    "and \"same\" provides the output size the same as the input size. \n",
    "\n",
    "Padding = $\\frac{f-1}{2}$\n",
    "F is usually odd because otherwise we need to have an asymmetrical padding. \n",
    "\n",
    "### Strided Convolutions\n",
    "Strided convolutions are anothe rbasic building block of convolutions. \n",
    "\n",
    "**Calulate size of output matrix when striding**\n",
    "\n",
    "$$[\\frac{n+2p-f}{s}+1] , [\\frac{n+2p-f}{s}+1]$$\n",
    "\n",
    "Technical note on cross-correlation vs. convolution:\n",
    "In signal processing applications we have a double mirroring operation on the filter before concolution. In DSP convolution means that there is this flipping of the filter operation and what is called convolution in deep learning is referred to as cross-correlation. This gives us the benefit of associativity but is not necessary for us in deep learning. \n",
    "\n",
    "### Convolutions over Volumes\n",
    "Convolutions over 3 dimentions. \n",
    "Create a filter that also has 3 dimensions, matching the number of channels as the input matrix. To compute the vlaue for the the ouput feed each color channel from the input inot the correspoindign channel in ther kernel. \n",
    "\n",
    "### Using multiple filters at once\n",
    "Maybe you want a horizontal and vertical edge detector. \n",
    "\n",
    "### Implement Convolutional Layer\n",
    "We use $f^{[l]}$ to denote the filter size of layer l\n",
    "\n",
    "$p^{[l]}$ to denote the padding\n",
    "\n",
    "$s^{[l]}$ to denote the stride\n",
    "\n",
    "$n^{[l]}_c$ to denote the number of filters\n",
    "\n",
    "The number of channels in the input should match the number of channels in the filter\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "Max Pooling has two hyperparameters, filter size and stride, but no parameters to learn\n",
    "\n",
    "There's is also average pooling. You might use average pooling somewhere in the middle of the neural network to decrease the size of the layer- otherwise you will usually see max pooling instrad of average pooling. \n",
    "\n",
    "### CNN Example\n",
    "\n",
    "When counting layers, the Conv layer and the max pool are considered one layer, when counting layers we only ocunt the weights, because nothign is being optimized in the max pool it is not a seperate layer. \n",
    "\n",
    "### Why Convolutions \n",
    "\n",
    "1. Parameter sharing \n",
    "2. Sparsity of connecitons\n",
    "\n",
    "Conv nets have very few parameters because the feature detectors, the weight sof the filter layers can be used to compute the entire input matrix despite have a few parameters. The same feature detector for the upper corner works for the lower corner. \n",
    "\n",
    "The other way they get away with so few parameters is that in each layer, each output value depends only on a small number of inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "### Case Studies\n",
    "ResNet- 152 layer netowrk with some tricks.  \n",
    "\n",
    "**Classic Networks**\n",
    "\n",
    "**LeNet 5 (1998)** MNIST, uses lots of average pooling, and a couple conv layers, it was small by modern standeds with avout 60k parameters. \n",
    "\n",
    "**AlexNet(2012)** For Imagenet, similar to LeNet but much bigger, had about 60 million learned parameters, used Relu where LeNet used just sigmoid and tanH\n",
    "\n",
    "**VGG16(2015)** Uses lots of Conv layers, and consistent maxpool and stride amounts. It was a very simple network with a very uniform architecture, part of what made it revolutionary. I38 million parameters. There is also VGG19 a bigger network\n",
    "\n",
    "### ResNets \n",
    "\n",
    "Skip connections take the activations from a network and move it to a later network, this allows you to deal with vanishing and exploding gradients- a problem in larger architectures. \n",
    "\n",
    "The skip connection connects to the next layer after the linear function but before the activation. \n",
    "\n",
    "### Why ResNets work so well\n",
    "\n",
    "At the very least with a skip connection the following layer will learn the identity of the previous layer. When the network is deeper and deeper it can be difficult even to learn the identy function. \n",
    "\n",
    "### 1x1 Convolution\n",
    "\n",
    "Does an element wise multiplication on a single positon across the channels of an input matrix. \n",
    "\n",
    "Also called a \"network in network\", created by lin et al(2013), it is used in the Inception network. \n",
    "\n",
    "**Why is this useful:** Imagine we have an input that has grown too large in size, if it has grown in width and height we can use a pooling layer to decrease the size of the matrix, but if the input has grown in depth of # of channels then we can use a 1x1 convolution to decrease it, say we have 28x28x192 be can use a convolution of 1x1x32 to create 28x28x32. \n",
    "\n",
    "### Inception Network\n",
    "\n",
    "Inception network uses several convolution layers and pooling layers with different size filters to construct the next layer. It uses a filter of 1x1, 3x3, 5x5, and a max pooling- it then concatenates all the results. Unfortunatley this is a large computational cost. In this example it creates 120M operations. Rather than choosing what size filter you want the inception layer computes a new layer with a variety of filter sizes and concatenates them. \n",
    "\n",
    "**Bottleneck layer:** Using a 1x1 convolution layer to shrink the representation before increasing the representation size. 12.4 million multiplications. The bottleneck layer will not hurt performance if it is implemented correctly, but it can save a lot of computations. A bottleneck layer is used wihtin the inception network of still doing all the operations of using a filter with 5x5, 3x3, etc. But using a 1x1 layer before each conv with different sized filter.\n",
    "\n",
    "The inception architecture is made up of 9 inception modules where there is a 1x1, 3x3, 5x5, max pool and bottlenecks. It also has these side chain layers with a softmax layer for prediction at two other points in the network. It helps ensure that even in the intermediate layers it has detection- it works as a type of regularization. \n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "Commmon Methods:\n",
    "1. Mirroring \n",
    "2. Random Cropping, cutting around the subject- not perfect\n",
    "3. Rotation\n",
    "4. Shearing\n",
    "5. Local warping \n",
    "6. Color shifting, add and subtract to color channels- draw changes from a distribution- sometimes \"PCA color augmentation is used\"- color PCA will keep the overall color tint the same\n",
    "\n",
    "### State of Computer Vision\n",
    "\n",
    "Object-detection: Finding an object and drawing a bounding box around it. \n",
    "\n",
    "On a spectrum of lots of hand engineering/little data to lots of data we have: object-detection, image recognition, and speech recognition. \n",
    "\n",
    "Complex architectures have grown out of the lack of data for computer vision tasks. \n",
    "\n",
    "Tips for doing well on benchmarks:\n",
    "1. Ensembling, train several networks independently and average their outputs. \n",
    "2. Multi-crop at test time, run classifier on multiple versions of the test images and average results. Use data augmentation on test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "### Object Localization (Bounding Box)\n",
    "Classification with localization means classifying the object and where in the picture the objects is, detection includes finding multiple objects in the picture. \n",
    "\n",
    "We can use a softmax layer to output the percenetage of confidence, the 4 points for the x,y corner and the height and width of the box, and a one-hot encoding of the classification. \n",
    " \n",
    "### Landmark Detection \n",
    "Capture some set of keypoints (landmarks). For instance find a set of 64 points along a face that map to the mouht and eyes. The labels have to be consistent across images.\n",
    "\n",
    "### Object Detection\n",
    "**Sliding windows detection:** Slide a window over the images, because each window has to be run through the neural net it is very \n",
    "inefficient. You can change the size of the window but it is still very costly. You can implement a **convolutional solution** that does not require the sliding window- thus saving on computation. \n",
    "\n",
    "### Bounding Box Predictions\n",
    "**YOLO:** Genrate a matrix y with the outputs including probability of object, bounding box, and a one hot classification. Yolo generates a Y for each of the target (window) cells. \n",
    "\n",
    "### Evaluating Object Localization\n",
    "Intersection over uniion, used when two bounding boxes overlap and used to decide if the predicted box is correct. If IoU is > 0.5 then the predicted box is considered correct. This metric is also used when comparing two bounding boxes.  \n",
    "\n",
    "### Non-Max Suppression\n",
    "When an object is detected multiple times non-max suppression is used to make sure the algorithm detects the object only once. When there are several bounding boxes- in non-max suppresion you take the rectangle with the highest probability and you compare it to overlapping boxes using IOU, you then suppress all the other boxes with lower probability. Applu non-max suppresion on each object class. \n",
    "\n",
    "### Anchor Boxes\n",
    "Anchor boxes are used when there are potentially multiple different classes within the same window. \n",
    "\n",
    "### YOLO Algorithm\n",
    "Yolo works by combinging each of the previous techniques we have looked at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "### Facial Recognition\n",
    "Face verification versus face recognition, the recognition problem is much harder- verfication is just about telling that the person is the perosn they say they are\n",
    "\n",
    "Recognition with a large amount of people can be difficult and computationally intesive. \n",
    "\n",
    "### One-shot Learning\n",
    "You might only have one pitcture of a face in a facial recognition task so you have to train a network that can learn to recofnize the photot with only a single instance. Rather than retraining the conv net the one-shot learning learns a 'similarity' function, d, that takes two images and outputs a large number if they are different and a lower number if they are the same. You compute similarity metrics for each pairwise comparison in the face database and use this to train face verification. \n",
    "\n",
    "### Siamese Network\n",
    "Comparing the encoding of two different inputs. Use a convolutional neural network to encode two different images of faces and then compare those faces. This distance between the encodings is the similarity metric. \n",
    "\n",
    "### Triplet Loss Function\n",
    "This is a loss function that can be used with facial detection. You pass it three images, an anchor images and a positive- meaning a positve example, for instance two different pictures of the same person. Then you pass it the anchor and a negaitve image, the same anchor with another image of a different person. This gives rise to the name, triplet loss, because you will be comparing three different images. Compare the squared norm of anchor and the positive, and the anchor and the negative, we then add alpha, a hyperparameter that acts as a margin- just like in SVM. If this value is les than or equal to some threshhold we decide whether these images are of the same person. \n",
    "\n",
    "$$\\ell(A,P,N) = max(\\|f(A)=f(P)\\|^2 - \\|f(A)-f(N) \\|^2 + \\alpha, 0)$$\n",
    "\n",
    "If you choose A,P,N are chosen randomly the triplet is easily satisfied. So you want to choose triplets that are hard to train on. \n",
    "\n",
    "### Face Recognition and Binary Classification\n",
    "You can use several similar formulas, like the chi-square similarity to compare two faces and create a binary classifer that verifies whether two different faces are the same. \n",
    "\n",
    "### Neural Style Transfer\n",
    "Take a content image and recreate it with the style of another image. \n",
    "\n",
    "### What are ConvNets learning\n",
    "Work from Fergus and Zeiler(2013), pick a unit in layer 1. Find the nine image patches that maximized the unit's activation, repeat for the other units. The result is the ability to see what images and patches most activate a set of neurons in the neural network. \n",
    "\n",
    "### Transfer Cost Function\n",
    "Use two different hyperparameters to balance how much of content and styel to use. Initialize image G randomly, then optimize as J(G). \n",
    "$$J(G) = \\alpha\\ J_{content}(C,G) + \\beta\\  J_{style}(S,G)$$\n",
    "\n",
    "### Content Cost Function\n",
    "Use a pretrained ConvNet like VGG and use some hidden layer l to compute the activations of the two images. If these two acitivations are similar then they must have simialr content. The const function for $J_{content}$ is the normalized squared norm of the activation of images C and G. This is a similarity metric of two image activations\n",
    "\n",
    "### Style Cost Function\n",
    "We can look at correlation of the generated and style image wihtin some channel to see how similar their syles are. \n",
    "\n",
    "Given an image generate a **style matrix**, the activations at some activations (i, j, k). In linear algebra these are called **gram matrix**. \n",
    "\n",
    "This is basically the **Forbenius Norm** being calculated on the two style image matricies. You get a better result when you compute the sytle cost function over several layers. \n",
    "\n",
    "### 1D and 3D Generalizations\n",
    "Many of the ideas discussed here apply to one dimensional and three dimensional data.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
