{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Evolution Strategies\n",
    "http://blog.otoro.net/2017/10/29/visual-evolution-strategies/\n",
    "## Hardmaru\n",
    "\n",
    "The credit assignment problem is the issue in RL of estimating the gradient for a reward signal given to the agent in the future to an action performed by the agent right now. \n",
    "\n",
    "There is a paper out of OpenAI called Evolution Strategies as a Scalable Alternative to Reinforcement Learning. While GAs are less data efficient it allows us to forgo having to calculate difficult gradients. It is also easier to parallelize computation for a GA to thousands of machines. They found that ES (evolution stategies) tend to be more diverse compared to polices discovered by RL algorithms. \n",
    "\n",
    "GA, RL, and ES can also be applied to search in the space of model architectures. \n",
    "\n",
    "Papers linked by Hardmaru:\n",
    "1. https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html\n",
    "2. https://arxiv.org/abs/1703.00548\n",
    "\n",
    "In this blog the author focuses only on application to seach for parameters of a pre-defined model. \n",
    "\n",
    "Schaffer and Rastrigin functions, two of several simple toy problems used for testing continuous black-box optimization problems. \n",
    "\n",
    "### Test functions for optimization \n",
    "https://en.wikipedia.org/wiki/Test_functions_for_optimization\n",
    "\n",
    "These benchmarks are used throughout the blog post to show different strategies and are worth understanding. \n",
    "\n",
    "### Simple GA\n",
    "A simple GA samples the parameter values from a distribution with a fixed standard deviation and a mean that begins at the origin and updates based on the fitness of candidate solutions. Given its greedy nature it throws away the best solutions, it can get stuck at a local optima,  and only really works on simple problems. It would be best to sample from a probability distribution that represents a more diverse set of ideas, rather than just from the best in the current generation. We can use **crossover** recombination, a process of sampling from 10% of the best perfomring solutions in the current generation, after recombinaiton we inject Gaussian Noise with a fixed standard deviation. \n",
    "\n",
    "However, in practice, most of the solutions in the elite surviving population tend to converge on a local optimum over time. The more sophisticated variations such as CoSyNe, ESP, NEAT cluster similar solutions together into different species to maintain better diversity over time. \n",
    "\n",
    "For disambiguation, it seem that ES and GA are different in that GA stresses crossover and mutation rather than simpler ES. \n",
    "\n",
    "### Covariance-Matirx Adaptation Evolution Strategy (CMA-ES)\n",
    "One of the most popular gradient-free optimization algorithms out there, and used by researchers and practioners alike. The most expensive part is the the covariance calculation, which is exponetial- though there have been recent approximations to make it linear. For Hardmaru it is the algorithm of choice if the parameters are under 1K. \n",
    "\n",
    "In Simple algorithms our standard deviation noise parameter is fixed. There are times we want to expand our search and times we want to converge on a given space. \n",
    "\n",
    "CMA-ES adaptively increases and decreases the seach space for the next generation. \n",
    "\n",
    "Here's how:\n",
    "1. Take the N_best solutions, say top 25%\n",
    "2. We use the best 25% to estimate the covariance matrix of the next generation, but the clever hack here is that it uses the current generation s mean rather than the updated generation's mean\n",
    "3. Reworded by me, we calculate the variance of the next generaiton with the data from the best of the previous generation. \n",
    "\n",
    "For a more in-depth description of the CMA-ES:\n",
    "https://arxiv.org/abs/1604.00772\n",
    "\n",
    "### Natural Evolution Strategies\n",
    "What if instead of looking for the max fitness value in the individual ant you optimize for traits and behaviors that benefit the entire ant population but focusing on the fitness sum of the ants. \n",
    "\n",
    "https://blog.openai.com/evolution-strategies/\n",
    "\n",
    "I misses many of the details but this seems like a great paper for exploring parallel training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Genetic Algorithms\n",
    "## Shiffman- Nature of Code\n",
    "\n",
    "Begin with John Holland in the 1950s. In 1956 and 27 year old Holland attends the Dartmouth Workshop.\n",
    "\n",
    "### Classic GA\n",
    "Box Car 2D- A flash based genetic algorithm \n",
    "\n",
    "Darwinian Natural Selection:\n",
    "Principles that must exist in the algorithm\n",
    "1. Heredity\n",
    "2. Variation\n",
    "3. Selection \n",
    "\n",
    "Important Design Principles:\n",
    "1. Fitness function \n",
    "2. How do you encode your DNA (genotype vs. phenotype)\n",
    "\n",
    "\n",
    "How to build the text generation tool\n",
    "1. Create a random population of N elements\n",
    "2. Calculate fitness for N elements\n",
    "    1. Fitness can be calculated as an exponetial relationship where a small change towards greater fitness is expressed as a greater chance of inheritance. \n",
    "3. Reproduction and selection\n",
    "    1. You can use two parents, 3, 4, whatever- one way is to only choose the top N parents. Another is to set the proportion of being chosen as a parent to the level of fitness, more fit candidates are more likely to be chosen but won't necessarily be chosen.\n",
    "        1. Use a pool selection to pick parent. Put everything in a bucket in proportion to it's fitness and grab at random. Normalize all the scores, sum the scores and divide each by the sum.  Pick a value randomly between 0-1, and subtract values starting with the first until you go below 0- use the value that dropped you below 0Z\n",
    "        2. Use rejection sampling, also called accept-reject algorithm, a type of monte carlo method. Chosee two numbers, one that represents the number of possible parents and one that represetns a threshhold value in the range of your fitness score. Choose the parent that maps to the random number and if the threshold value is less than the fitness of that parent accept it as a parent, ortherwise reject.  \n",
    "    2. Combine the pieces of the parents:\n",
    "        1. Take each alternating letter from each parent\n",
    "        2. Pick a pivot at random and take parts from the parents in accordance with that pivot. \n",
    "    3. Add mutation to the children to compensate for the lack of diversity in the initial population.  \n",
    "    4. Interactive Selection. Use a fitness function that is based on interaction from the user. A lot of Karl Sims stuff uses this. \n",
    "\n",
    "2. Continuous Evolutionary System\n",
    "Evolutionary algorithm applied to a continuous system of adaptaiton and evolution- fitness in this context is not a score but a product of whether or not the organism survives and lives on to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroEvolution - A new kind of deep learning\n",
    "[link to article](https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning\n",
    "https://nips.cc/Conferences/2017/Schedule?showEvent=8744e )\n",
    "\n",
    "The concern in neuroevolution focuses on the origin of the architecture of the brain itself. The writier makes it seem like Neuroevolution is most concerned with the architecture of the brain to begin with. The first Neuroevolution algorithms befin in the 1980s as an alternative to the more conventional backpropogation. Early neuroevolution seemed much like deep learning today- where researchers chose the architecture and let the evolution evolve the weights, this was called fixed-topology neuroevolution. Scientists would breed 100s, for ex, fixed topology network, seeing which networks performed best and deciding from that which ones to breed. \n",
    "\n",
    ">where several important fixed-topology algorithms had already been invented (such as the SANE and ESP algorithms from David Moriarty and Faustino Gomez, both in collaboration with Risto Miikkulainen). These algorithms were already quite clever, investigating the idea that the individual neurons of a network might be evolved in their own subpopulations to cooperate with other neurons, and then grouped with neurons from other subpopulations to form full working networks, an idea generally known as “cooperative coevolution.”\n",
    "\n",
    "The author was looking for a neuroevolution algorithm that could 'explode in complexity' - the result is the creation of NEAT, NeuroEvolution of Augmenting Topologies. \n",
    "\n",
    "#### Novelty Search\n",
    "\n",
    "One finding, is that evolving the most fit parents does not always yield the best long-term result. Instead looking for the most novel approaches can yield the best long-term success. This is interesting given the recent paper DIAYN that uses max entropy to look for the most unique behaviors. This led to the development of a [**novelty search algorithm**](http://eplex.cs.ucf.edu/noveltysearch/userspage/) This algorithm led to the development or a research area, 'quality diversity'. \n",
    "\n",
    "**Algorithms to look into:** NEAT, HyperNEAT, CCPN, and novelty search.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
