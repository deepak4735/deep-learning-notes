{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Model-Based Reinforcement Learning\n",
    "### Chelsea Finn\n",
    "\n",
    "## Why Use Model Based Approaches \n",
    "Sample efficiency. \n",
    "\n",
    "Comparing different algorithms we've seen. Fully online methods are about 10x as efficient compared to gradient-free methods. Fully online to replay-buffer/value estimation you get about 10x improvement in terms of sample efficiency. You see another 10x gap going to model-based RL (if you are using linear models). \n",
    "\n",
    "Use model-based approaches because they are more sample efficient and are more transferable across tasks. If you learn a policy that policy is great for one tasks, if you are able to predict the environment then that can be used with many takss within the environment. \n",
    "\n",
    "In a model based approach you fit this model $p(s'|s,a)$ the transistion function, and optimize the model $\\pi_\\theta (a|s)$. \n",
    "\n",
    "1. Run bas\n",
    "3. Backprop to choose e policy, can be random search, and collect $D$\n",
    "2. Learn model $f_\\phi$\n",
    "3. Backpropagate through model into a policy to optimize \n",
    "4. Run new policy and append new states and learn another model\n",
    "\n",
    "Design a good base policy, particularly effective if we can hand-engineer a dynamics representation using our knowledge of physics, and fit just a few parameters. \n",
    "\n",
    "## Model-Predictive Control \n",
    "1. Run policy\n",
    "2. Fit model\n",
    "3. Backprop to choose actions\n",
    "4. Execute first action and observe the resulting state\n",
    "5. Append to data set (go to step 3)\n",
    "\n",
    "## Main Approaches\n",
    "1. Gaussian Process\n",
    "2. Neural network \n",
    "3. Other \n",
    "\n",
    "## Local vs. Global models\n",
    "A problem with model-based approaches is that the planner will seek out regions where themodel is erroneously optimistic. \n",
    "\n",
    "In some instances a task is much more complex than the model- \n",
    "There is an important relationship betwwen the global and local model- for isntance in the example used bycompre the speaker, you can use the global model as the prior to the local model and use the local model to update the policy. \n",
    "\n",
    "I think this has something to do with a controller- there is a part of the lecture that gets into linear-Gaussian controllers\n",
    "\n",
    "### Guided Policy Search\n",
    "Uses a global policy to imitate multiple local policies that can create a better global policy. \n",
    "\n",
    "### Learning in Latent Space\n",
    "We learn an embedding g(o_t) from a latent space that we then apply \n",
    "to traditional model based approach. \n",
    "\n",
    "Simialar to the Hardmaru World Models paper it is convention in the field to use autoencoders/VAE to compress a world and see how the algorithm moves in that world and then seeing if it can reconstruct the world the image. \n",
    "\n",
    "We relearn a feature for each activity\n",
    "Low dimensional embeddings used in this approach are also useful for model-free approaches. A good latent spac ecan be used to learn for efficiently even for algorithms that are already very efficient. \n",
    " \n",
    "Pro\n",
    "- Learn complex visual skills efficiently\n",
    "- Structured representation enables effective learning\n",
    "\n",
    "Cons\n",
    "- REconstruction objectives might not recover the right representation \n",
    "\n",
    "### Modeling directly in observation space\n",
    "\n",
    "Pros\n",
    "- Entirely self-supervised\n",
    "- Learn for a variety of task \n",
    "- More efficient than single-task model-free learning\n",
    "Cons\n",
    "- Can't yet handle as complex skills as in model-free methods\n",
    "\n",
    "## Suggested Reading for Model-Based RL\n",
    "Tassa et al. Sythesis and Stabilization of Complex Behaviors\n",
    "Levine, Finn, Enf-to-End Learning of Deep Visuomotor Policies. \n",
    "Heess at al, Stochastic Value GRadients\n",
    "Embed-to-Control, learn latent space and use model-based rl \n",
    "\n",
    "## Model-Based vs. Model-Free \n",
    "Models\n",
    "- Easy to Collect data in a scalable way\n",
    "- Possibility to transfer across tasks\n",
    "- Typically require a smaller quantity of supervised data\n",
    "- Models don't optimize for task performance \n",
    "- Sometimes harder to learn than a policy\n",
    "- Often need assumptions to learn complex skills\n",
    "\n",
    "Model-Free\n",
    "- Makes little assumptions beyond a reward function\n",
    "- Effective for learning complex policies\n",
    "- Require a lot of experience\n",
    "- Not transferrable across tasks\n",
    "\n",
    "## Closing Notes\n",
    "Model-based rl is an under-explored area of research\n",
    "\n",
    "Two active, exciting areas:\n",
    "- model based approaches with high dimensional observations\n",
    "- combining elements of model-based planning and model-free policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10a: Utilities\n",
    "### Pieter Abbeel\n",
    "\n",
    "We optimize expected reward. Utilities are functions from outcomes/state to real numbers that describe an agent's preferences\n",
    "\n",
    "Utilities come from the need to dictate what the agent does without describing how it does that thing. \n",
    "\n",
    "## Rational Preferences\n",
    "1. Orderability\n",
    "2. Transitivity\n",
    "3. Continuity\n",
    "4. Substitutabilitu\n",
    "5. Monotonicity\n",
    "\n",
    "Theorem: Rational preferences imply behavior describable as maximizing expected utility. \n",
    "\n",
    "This lecture is about different utility functions that you can use to parameterize a model with different goals. There is so time spent on microMorts and other measures that might be used in healthcare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10b: Inverse Reinforcement Learning\n",
    "### Chelsea Finn\n",
    "\n",
    "## Reward Function \n",
    "\n",
    "### Behavioral Cloning/ Direct Imitation Learning\n",
    "No reasoning about dynamics or outcomes \n",
    "The expert might have different degrees of freedom. \n",
    "\n",
    "**Inverse Optimal Control: Infer the reward function from demonstrations**\n",
    "\n",
    "Input is state eand action space, rollouts from optimal policy, or sometimes dynamics model \n",
    "\n",
    "The goal is recover the reward function that the agent was acting on and then use the reward to get policy. \n",
    "\n",
    "**Challenges:**\n",
    "Undefined problem, difficult to evaluate a learned reward. Demonstrations may not be optimal. \n",
    "\n",
    "### Maximum Entropy Inverse RL\n",
    "Handles ambiguity using probabolistc model of behavior\n",
    "\n",
    "We have \n",
    "-Tau, a set of trajectories\n",
    "-R, total reward parameterized by psi summed over all trajectories\n",
    "-D, the set of expert demonstrations that are sampled from the expert policy \n",
    "\n",
    "$p(\\tau) = \\frac{1}{Z}exp(R_\\psi(\\tau))$\n",
    "\n",
    "Infer reward max logliklihood, this is an energy-based model of behavior. \n",
    "\n",
    "Has some relation to advesarial networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
