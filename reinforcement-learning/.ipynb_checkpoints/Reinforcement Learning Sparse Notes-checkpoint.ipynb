{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning is the science of making optimal decisons.** \n",
    "\n",
    "There are two fundamental difficulties one encounters while solving RL problems: the balance of exploration vs. exploitation and long term credit assignment.\n",
    "\n",
    "Here are the parts in a fictional scenario with a kid who is studying for a test:\n",
    "1. Agent - Kid\n",
    "2. Environment- Reward system and exam\n",
    "3. States - Topics\n",
    "4. Value-Function - Which topics to give importance to \n",
    "5. Reward\n",
    "6. Policy- Method kid uses to complete the topics within time.\n",
    "\n",
    "This is different than other machine learning paradigms:\n",
    "1. There is no supervisor present.\n",
    "2. Importance of time. Reinforcement Learning pays much attention to sequential data unlike other paradigms where you recieve random inputs. Here the input at the next step will always be dependent on input at the previous state.\n",
    "3. Concept of delayed rewards. You may not get a reward at each step. A reward may be given only after the completeion of the entire task. Also, say you get a reward at a step only to find out that you made a huge blunder in a future step. \n",
    "4. The agents action effects its next input. If you have the choice of going either left or right, after you take the action, the input at the next time step will be different if you chose right rather than left. \n",
    "\n",
    "History describes all the events that have taken place between the environment and the agent. The history is called state. \n",
    "$S_t = f(H_t)$. $S^a_{t}$, $S^e_{t}$  Represents the agent and environment respectively. \n",
    "\n",
    "Markov State. A Markov state stores all the informaiton from the past in an abstract form. If we want to predict the future, rahter than using the whole history we can use the Markov State. The Markov State has no less information. \n",
    "\n",
    "There are partially and fully observable environments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. \n",
    "\n",
    "We will develop a table where the rows are states the columns are actions we can take. The state-action table represents the environment and we initilaize all to zero. We use the bellman equation to update the table. $Q(s,a) = r + y(max(Q(s', a'))$ \n",
    "\n",
    "s is state, s' is the resulting state. a is action, Y is the discount factor, r is reward. So the Q value is the sum of the instant reward andthe discounted future reward(value of the resulting state). The discount factor 'y' determines how much importance you want to give to future rewards. \n",
    "\n",
    "Each iteration made by the agent is referred to as an episode. For each episode it will try to achive the goal state.\n",
    "\n",
    "**Q-learning attempts to learn the value of being in a given state, and taking a specific action there. Policy gradient s attempt to learn functions which directly map an observation to an action**\n",
    "\n",
    "Q-learning is a table-based method of finding rewards, once our environment grows until it can no longer be handled by a table we use a neural netowrk as a function approximator. \n",
    "\n",
    "For qlearnig, while neural networks allow for greater flexibility it comes at the cost of stability. Two tricks we could add are called **experience replay** and **freezing target networks**. Both of these techniques comes from the RL atari palying ai and are discussed in several blogs like: http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\n",
    "\n",
    "**N-Armed Bandit**: There are n many slot machines with differnet fixed payout probability, The goal is to discover the machine with the best payout- and maximize the returned reqard by always choosing it. The N armed bandit is a nice starting point because we don't have to think about any movments around a board. All we need to focus on is learning which reaeds we get for each of the possible actions and ensure we choose the optimal ones, this is called learning a policy. Upgrading out gradients in the neural net in this context is called policy gradients. There is another approach where agents learn value functions, In those approaches, instead of learning the optimal action in a given state, the agent  learns to predict how good a given state or action will be for the agent to be in. \n",
    "\n",
    "\n",
    "**Value Function v Policy GRadient**\n",
    "\n",
    "Policy gradients are a view with a direct connection between action and outcome bereft of a sense of *field* I believe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 Deep Reinforcement Learning\n",
    "\n",
    "1. What is reinforcement learning?\n",
    "2. Markov decision process\n",
    "3. Q Learning\n",
    "4. Policy gradient\n",
    "\n",
    "Car Pole Problem\n",
    "Objective: Balance pole on movable cart\n",
    "State: Angle, angular speed, postion, horizontal veolicity\n",
    "Action: horizontal force applited on the cart\n",
    "Reward: 1 at each time step if pole is upright\n",
    "\n",
    "Robot locomotion\n",
    "Objective: Move forward\n",
    "State: Angle and positon of joints\n",
    "Action: Torques applied on joints\n",
    "Reward: 1 at each time step uprights + forward movement\n",
    "\n",
    "Atari Games\n",
    "Objective: Complete the game with the highest score\n",
    "State: Raw pixel inputs of the game state\n",
    "Action: Game controls e.g. Left, Right, Up, Down\n",
    "Reward: Score increase/ decrease at each time step \n",
    "\n",
    "Go\n",
    "Obj: Win the game\n",
    "State: Position of pieces\n",
    "Action: Where to put next piece down\n",
    "\n",
    "Markov Decision Process is the mathematical formaulaiton of the RL problem. Markov Property: current state completely characterises the state of the world. \n",
    "\n",
    "Defined By (S, A, R, P, Y)\n",
    "s: set of possible states\n",
    "a: set of possible actions\n",
    "r: distribution of reward given (state, action) pair\n",
    "p: transition probability, distribution over next state given (state, action) pair\n",
    "y: discount factor\n",
    "\n",
    "**Markov Property:** Current state compeletely characterises the state of the world \n",
    "\n",
    "**Markov Property2:** $P(X_t = x_t| X_{t-1} = x_{t-1})$ The future value of the random variable is independent of everything in the past. It's value and the probability of taking on any particular value only depends on the present state of the system. \n",
    "\n",
    "### Markov Decision Process\n",
    "From t=0 until done:\n",
    "    1. Angent selects an action a1\n",
    "    2. Environment samples reward rt\n",
    "    3. Environment samples next state \n",
    "    4. Agent revienves reward rt and next state st+1\n",
    "    \n",
    "A policy $\\pi$ is a function form S to A that specifes what action to take in each state\n",
    "\n",
    "Objective: find policy n* that maximizes cumulative discounted reward, the sum of the future reward discounted by the discount factor \n",
    "\n",
    "Optimal policy is a policy that at every possible state takes us closest to a terminal state. \n",
    "\n",
    "We want to find pi*. How do we handle reandomness, initial state, transition probability. Maximize the expected sum or rewards. \n",
    "\n",
    "Pi* has a formal definition: \n",
    "\n",
    "![optimal policy](../../assets/optimalPolicy.jpg)\n",
    "\n",
    "Definitions we need:\n",
    "**Value funciton**: at state S is the cumulative reward from following the policy from state s\n",
    "**Q-value function**: How good is a state action pair, the expected cumulative reward for taking state S and following the optimal policy\n",
    "\n",
    "\n",
    "![bellman](../../assets/bellmanEquation.png)\n",
    "\n",
    "\n",
    "An important part of where the Bellman equaiton fails is that you have to compute it for eveyr s,a pair. It is not scalable. The solution is to approximate the function with a neural network. This is deep q-learning. \n",
    "\n",
    "![bellmanDerivative](../../assets/bellmanDerivative.png)\n",
    "\n",
    "Architecture of Atari Game\n",
    "Q-network architecture:\n",
    "16, 8x8 conv, stride 4\n",
    "32, 4x4 conv, stride 2\n",
    "Fully - Connected 256\n",
    "FC-4(Q-Values)\n",
    "The last layer has 4-d output corresponding to Q(st, a1, a2, a3, a4)\n",
    "\n",
    "Lessons: \n",
    "**Exerpeience Replay**: Learning from consecutive samples is bad because playing the game and taking smaple means that the moves are correlated and this leads to inefficient training. If the optimal move is move left, training samples will be dominated by samples from left-hand size and can lead to bad feedback loops. \n",
    "\n",
    "The solution is expereince replay, we continually update a replay memory table of transitions as game expisodes are played. We train !-netowrk on random minibatches of transistions from replay memory, instead of consecutive samples. We can sample on table multiple times- which is more efficient computationally.  \n",
    "\n",
    "The problem with Q learnig is that the Q function is very complicated, in a high dimensional state its hard to learn every state, action pair that you have. But a policy can be much simpler, a policy is a single step, can you just focus on the policy rather than focusing on finding the q values then its much quicker- we can just learn a policy directly from a collection of policies. \n",
    "\n",
    "For each policy define a value, and we want to find the optimal policy. We can use gradient descent. ---then there's a bunch of math --- \n",
    "\n",
    "Can we compute those quantities without knowing the transition probabilities of the next state that we get \n",
    "\n",
    "We do not need to know our transition probabilites to compute our gradiennt, when sampling trajectory r, we can estimate \n",
    "\n",
    "If the reward is high we want push up the probabilites of the actions we took- if the reward is low we want to push down the probabilities of the actions we took. \n",
    "\n",
    "It might seem simplistic to do this but on average it works out. \n",
    "The problem is that it suffers from high variance- we need a lot of samples. Variance reduction is an important area of research in polivy gradients and ways we can improve the estimator. \n",
    "\n",
    "**Two Solutions:**\n",
    "1. Push up probabilites of an action see, only by cumulative future reward from that state\n",
    "2. Use discount factor to ignore delayed effects. Focus on rewards soon and downgrade the ones that come later on. Look at local neighborhood\n",
    "3. Use a baseline: A simple baseline- constant moving average of rewards experienced do far from all trajectories. Raw trajectories don't mean much if you end in reward often - you just push up to various degrees the same things. Instead use a baseline, what do we expect to get from this state and now our reward/scaling factor is subtracted by our baseline. \n",
    "\n",
    "These varinace techniques are used in vanilla reward. \n",
    "\n",
    "How do we choose a better baseline? If the action was better then the expected action- this is like our value functions. \n",
    "\n",
    "**Actor-Critic Algorithm**\n",
    "We can combine policy gradients and q-learning by trainin goth an actor(a policy) and a critic(a q function). \n",
    "\n",
    "- The actor decides which action to take, and the critic tells the actor how good its action was and how it should adjest. \n",
    "- The critic only has to learn the values of state, action pairs generated by the policy.\n",
    "- Can also incorporate q-learning tricks, experience replay\n",
    "- remark: we can define by the **advantage function** how much an action was better than expected. \n",
    "\n",
    "Requires sample efficiency. \n",
    "\n",
    "**Summary**\n",
    "Policy gradients: very general but suffer from hgh varinace so requires a lot of smaples. Challenge: sample-efficiency\n",
    "q-Learing: does not always work but when it works, usually more sample efficient. Challenge: exploration\n",
    "\n",
    "Garuntees: \n",
    "Policy-gradients: converges to a local minima of J, often good enough\n",
    "q-Learning: Zero garuntees since you are approximating Bellman equation with a complicated function approzimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
