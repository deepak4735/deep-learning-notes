{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard \n",
    "\n",
    "Tensorboard allows us to track our variables and visualize the structure of the graph. \n",
    "\n",
    "Here a quick list of how to implemnt Tensorboard: \n",
    "1. Put nodes that you want represented in the graph inside a tf.name_scope context manager and add name=\"\" in the variable signatures to name them in the graph visualization. There are two kinds of scope that we'll get into later. All nesting in the naming will be represented in the graph later.\n",
    "2. Decide what values you want to log and write summaries for them, there are scalar, and histogram summaries. Merge the summaries into an operation that will get called in sess.run along with the variables that will be written, then write those summaries to a FileWriter object.\n",
    "3. Setup a Saver() object that will be used for saving the graph at points during the training that the user decides.\n",
    "\n",
    "Values are tracked in Tensorboard via summary nodes. There are two types of summary nodes, scalar summaries `tf.summary.scalar` and `tf.summary.histogram`. The former get tracked by a two-dimensional plot over time and the latter produces a graph visualizing the distribution of values over time. \n",
    "\n",
    "Namespace nodes with `tf.name_scope`. Once you have nodes named and you have tracked the values you'd like to watch then write sumaries to disk with `tf.summary.FileWriter` and initialize it with the path to dump data and the graph to display like: \n",
    "\n",
    "```python\n",
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter('/tmp/log', graph=session.graph)\n",
    "```\n",
    "\n",
    "Then you'll want to merge your summaries into one operation, as well as have a global step variable to track your epochs with some code like this:\n",
    "\n",
    "```python\n",
    "merged = tf.summary.merge.all()\n",
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter('/tmp/log', graph = session.graph)\n",
    "    for step in range(100):\n",
    "        writer.add_summary(merged.eval(), global_step=step)\n",
    "```\n",
    "\n",
    "You can launch a tensorboard serve with:\n",
    "`tensorboard --logdir=/tmp/log`\n",
    "\n",
    "#### Read incase of emergency\n",
    "To get a correct graph representation you should stop tensorboard and jupyter, delete your tensorflow logdir, restart jupyter, run the script, and then restart tensorboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope and Graph Visualization\n",
    "\n",
    "Use Scopes to define subgraphs for readibility\n",
    "\n",
    "`tf.name_scope`\n",
    "`tf.variable_scope`\n",
    "\n",
    "Variable sharing in tensorflow is a mechansim that allows for sharing variables in different parts of the code without passing references to the variable around. \n",
    "\n",
    "With tensorboard scopes allow users to create semantically meaninful groupings for our models. We have two scope options: `tf.name_scope` and `tf.variable_scope`. Both scopes have the same effect of adding the name to the variable prefix but name scopes will be ignored by the `tf.get_variable` function. \n",
    "\n",
    "`tf.get_variable` ignores a variable's name scope which allows us to rettrieve a variable that exists somewhere else in the graph and use it under a different name scope but it will still be the same variable in the same variable scope. \n",
    "\n",
    "https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n",
    "\n",
    "**Do NOT use tf.name_scope and tf.Variables with shareable variables, always use tf.variable_scope to define the scope of a shared variable. Then, use tf.get_varaible to create or retrieve a shared variable.**\n",
    "\n",
    "Notes about how the graph is rendered: \n",
    "    \n",
    "Training block attaches to eveyrthing so that's why it's connected with everything.\n",
    "\n",
    "The graph finds repeated similiar sub strucutres of the graph and gives them the same colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Experiments\n",
    "\n",
    "## Saving checkpoints\n",
    "Because models can take several days to run we need to build a system that is invariant to our computer accees- if we have to stop a model in the middle of training or if the computer crashes, etc. \n",
    "\n",
    "To save our models we will leverage `tf.train.Saver()`\n",
    "\n",
    "`.Saver()` allows us to same our graph as binary files that we can restor from. \n",
    "\n",
    "```python \n",
    "# define model\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# launch a session to compute the graph\n",
    "with tf.Session() as sess:\n",
    "    for step in range(training_step):\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoint_directory/model_name', global_step = global_step)\n",
    "```\n",
    "The step at which you save your graph's variables is called a checkpoint. \n",
    "\n",
    "#### Restoring from checkpoint\n",
    "1000 in the name refers to the step at which  checkpoint was made\n",
    "`saver.restore(sess, 'checkpoint/mode-1000')`\n",
    "\n",
    "You should check if there is a checkpoint before you load it like this\n",
    "\n",
    "``` python \n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "if ckpt and ckpt.model_checkpoint:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "```\n",
    "\n",
    "We can also pass in variable names if you only want to save some variables in a graph\n",
    "\n",
    "**If you save your summaries into different sub-folders in your graph folder you can compare your progresses**\n",
    "\n",
    "## Reproducing experiments\n",
    "It is important for other researchers to be able to reproduce our experiments, this requires us to control for random variation in how we initialize weights or shuffle the order of training samples. \n",
    "\n",
    "We can use something similar to `random_seed` and `random_state` in numpy to control randomization\n",
    "\n",
    "1. Set random see at operation level. \n",
    "`my_var = tf.Variable(tf.truncated_normal((-1.0,1.0, stddev=0.1, seed=0))`\n",
    "\n",
    "2. Set a random seed at graph level with tf.Graph.seed\n",
    "`tf.set_random(seed)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Restoring Models\n",
    "\n",
    "```python\n",
    "saver =  tf.train.import_meta_graph(\"graph name\")\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    " \n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    " \n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    " \n",
    "print sess.run(op_to_restore,feed_dict)\n",
    "#This will print 60 which is calculated \n",
    "#using new values of w1 and w2 and saved value of b1. \n",
    "```\n",
    "   \n",
    "Here's an example where we are importing a model  VGG but just re-training the last layer\n",
    "\n",
    "```python\n",
    "saver = tf.train.import_meta_graph('vgg.meta')\n",
    "# Access the graph\n",
    "graph = tf.get_default_graph()\n",
    "## Prepare the feed_dict for feeding data for fine-tuning \n",
    " \n",
    "#Access the appropriate output for fine-tuning\n",
    "fc7= graph.get_tensor_by_name('fc7:0')\n",
    " \n",
    "#use this if you only want to change gradients of the last layer\n",
    "fc7 = tf.stop_gradient(fc7) # It's an identity function\n",
    "fc7_shape= fc7.get_shape().as_list()\n",
    " \n",
    "new_outputs=2\n",
    "weights = tf.Variable(tf.truncated_normal([fc7_shape[3], num_outputs], stddev=0.05))\n",
    "biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "output = tf.matmul(fc7, weights) + biases\n",
    "pred = tf.nn.softmax(output)\n",
    " \n",
    "# Now, you run this with fine-tuning data in sess.run()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
