{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives\n",
    "\n",
    "### Contour Plots\n",
    "Show all the points where f(x,y) are constants, usually chosen at regular constants. Take the equation of a multivariate shape and for constant values of the function graph the coresponding values of x, y. \n",
    "\n",
    "### Approximation formula\n",
    "If we change x by some amount delta x, and y by some delta y, then $\\Delta z \\approx f_x \\Delta x + f_y \\Delta y$\n",
    "\n",
    "We can justify this formula with tangent plane formula. Two lines are tangent to the graph and determine a plane. \n",
    "\n",
    "### Max/Min optimization problems\n",
    "At a local min or max, both f'(x) and f'(y) are 0. This is the same as saying the tangent plane to the gfaph is going to be horizontal. This point is a **critical point** if all the partial derivatives are zero at the same time. Once you have set your system of equations to 0 and have discovered the critical point, you then have to determine if it is a maximum or a minimum. There is a third kind of critical point- a saddle point.\n",
    "\n",
    "A **saddle point** is neither a maximum or a minimum, because it can be both depending on the direction. When you have two squared terms you will get saddlepoints because the curvature will change depending on if you negative or positive values. \n",
    "\n",
    "### Least Squares Interpolation\n",
    "For various values of x you value what y ends up being. Given the experimental data (x_1, y_1), (x_2, y_2)... Find the 'best fit' line of y= mx + b. Finding the best a and b you minimize total square distribution. \n",
    "\n",
    "### Second derivative test\n",
    "How to decide between max, min, or saddlepoint. If the question is how do we the global max/min. We can use the second derivative test. The second derivative will tell you if it's curving up or curving down.  \n",
    "\n",
    "**degenerate critical point**- I line along the graph where nothing happens- a sort of chasm where the line along the chasm in the graph is all critical points. You can have degenerate saddlepoints. \n",
    "\n",
    "Notation for the second derivative: \n",
    "$$\\frac{\\partial^2{f}}{\\partial{x^2}} = f_{xx}$$\n",
    "\n",
    "$$\\frac{\\partial^2{f}}{\\partial{x}\\partial{y}} = f_{xy}$$\n",
    "\n",
    "F_xy and F_yx will always be equal to eachother. \n",
    "\n",
    "At a critical point (x_0, y_0) of f, let A = f_xx(x_0, y_0), B = f_xy(), and C= f_yy(). \n",
    "\n",
    "If AC-B^2 > 0 and A > 0, local minimum. \n",
    "If AC-B^2 > 0 and A < 0, local maximum. \n",
    "If AC-B^2 < 0 saddle point.\n",
    "If AC-B^2 = 0 cannot conclude. \n",
    "\n",
    "### Quadratic approximation\n",
    "Quadratic approachimations are a specific class of Taylor polynomials. We can solve for critical points at 0 for (1) the original function, (2) the first derivative, and (3) the second derivative. Take the results for each of these critical points at each of the derivatives and plug them into the normal form of the quadratic to approximate the function at the second derivative. In functions that cannot be differentiated we can use the quadratic approximation to identify min/ max. \n",
    "\n",
    "### Implicit Differentiation\n",
    "Implicit differentiation is when you want to take the derivative implicitly rather than defining the equation in terms of y = x. An example would be taking the derivative of $x^2 + y^2 = 1$. We apply the derivative operator to both sides of this equation and then apply the chain rule. \n",
    "\n",
    "It's called implicit differentiation but it's really just an application of the chain rule. In an example like the above problem you will often end up with a term $\\frac{dy}{dx}$ \n",
    "\n",
    "### Total Differential \n",
    "$$f(x, y, z)$$\n",
    "$$ df = f_xdx + f_y dy + f_z dz$$\n",
    "\n",
    "Can also be written with \\partial notation. These are differentials- they can only be expressed in terms of other differentials. They are placeholders where you can put other things. \n",
    "\n",
    "We can use the total differential when we have some function $w = w(x, y, z)$ where functions x, y, and z depend on some variable t. We can find dw/dt = w_x dx/dt + w_y dy/dt + w_z dz/dt. Here the partial derivaties indicate the changes to the the values W. We can write this as \n",
    "$$\\Delta_w = <w_x, w_y, w_z>$$\n",
    "\n",
    "Where the w's are the partial derivatives.We can talk about the gradient of W at some point x, y, z. So at each point we get a corresponding vector and the totality of these vectors are a vector field. \n",
    "\n",
    "### Directional Derivatives\n",
    "This is a gradient vector\n",
    "$$\\Delta_w = <w_x, w_y, w_z>$$\n",
    "\n",
    "Theorem: $\\Delta_w \\bot \\ level\\ surface\\ ,w=constant$\n",
    "Say we have a contour plot, if we have a point on the contour plot the gradient vector will be a vector perpendicular to the line on the graph at that point. The gradient vector is perpendicular to the contour lines- the countour lines represent some consistent interval plugged into the function. \n",
    "\n",
    "A directional derivative is a new kind of derivative, the same way that a partial derivative is taken with respect to the some input variable, the directional derivatice is taken along some vector in the input space. The directional derivatice is the resulting rate of change in the output of the function.\n",
    "\n",
    "It has several notations:\n",
    "$$\\nabla_{\\vec{v}}f$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\vec{v}}$$\n",
    "\n",
    "We compute the directional derivative with the gradient of our function f, and take the dot product of our vector.\n",
    "$$\\nabla f \\cdot \\vec{v}$$\n",
    "\n",
    "The gradient f is a vector full of partial derivatices with respect to each of the coordinate directions.  \n",
    "\n",
    "Some people divide the directional derivative by the maginitude of the vector v like: \n",
    "$$\\frac{\\nabla f \\cdot \\vec{v}}{||\\vec{v}||}$$\n",
    "\n",
    "So we have computated the directional derivative in the direction of the unit vector.  \n",
    "\n",
    "### Lagrange Multipliers\n",
    "The goal is to minimize or maximize a function with several variables, where x, y, z are not independent. We are trying to minimze this function under some constraint. For instance we can consider an economics problem where we have a multivariate function that takes, labour and goods and outputs revenue- and we want to maximaize that function according to some budget. If we plot our multivariate function and the line representing our constraint- we want to find the point where our function just touches the constraint. We want to find the point where the gradient of function is on the same line as the gradient of our constriant, written as \n",
    "$$\\nabla R = \\lambda \\nabla g $$\n",
    "\n",
    "Where the lambda is called our Lagrange multiplier. We compute the partial derivaties of R with respect to its own inputs and take the  partial derivatives with respect to the constraint g. We use the partials in the above formula to declare the relationship between the variables, where the gradients are proportional. The langrange multiplier is a way to package up the three equations in a way that won't necessarily help you much by hand but is much easier to hand a computer. \n",
    "\n",
    "#### The Langrangian\n",
    "Given a function R that takes x,y and a constraint B that takes x,y and equals b. Here is the Lagrangian \n",
    "\n",
    "$$\\mathcal{L}(x, y, \\lambda) = R(x,y) - \\lambda(B(x,y) - b)$$\n",
    "\n",
    "When you take the gradient of the Lagrangian and set it to zero, we  encapsulate all three equations that we need.  The computer can solve the constrained maximization problem with the lagrnagian very easily, by basically making it an unconstrained maximization problem where $\\nabla b = 0$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
