{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Product\n",
    "The dot product. The dot product works because we can express our vector, v, in terms of the unit vectors where the unit vectors are the two other sides to a triangle with hypotenuse vector v. Vector V dot vector v equals the square of its magnitude. The dot product is associative, commutative, and distributive over scalar multiplication. \n",
    "\n",
    "#### Cosine Rule\n",
    "$$\\vec{u}\\cdot \\vec{v} = |u||v|\\cos(\\theta)$$\n",
    "\n",
    "#### Angle Between Vectors\n",
    "$$\\cos(\\theta)= \\frac{\\vec{u}\\cdot \\vec{v}}{|u|\\cdot|v|}$$\n",
    "\n",
    "#### Scalar Projection\n",
    "Scalar projection is the name for the projection of one vector onto another- derived from the cosine rule. The scalar projection of vector u onto vector v is:\n",
    "\n",
    "$$|u|\\cos(\\theta)$$\n",
    "Or\n",
    "$$\\frac{\\vec{u} \\cdot \\vec{v}}{|v|}$$\n",
    "\n",
    "\n",
    "This is also why the dot product is called the projection product. The projection is how much the projected vector collapses onto the line parallel to the other vector, scalar projections can be positive, negative, or zero.\n",
    "\n",
    "#### Vector Projection\n",
    "The vector projection is the scalar projection times the vector divided the vector magnitude of the projected onto vector.\n",
    "Vector u onto vector v is:\n",
    "\n",
    "$$\\frac{v \\cdot u}{v \\cdot v}{v}$$\n",
    "\n",
    "#### Changing Basis \n",
    "When basis vectors, the constiuent unit vectors that compose our other vectors are orthogonal- it makes our vector computations mucn more efficient. We can use different sets of basis to project our vectors into new coordinate systems. Our vector will be $r_u$ in the basis u, and the new basis $v_1$ and $v_2$.\n",
    "1. Check that the new basis vectors are orthoganol, take the dot product and divide by their lengths, if its 0- then they are orthoganol. \n",
    "2. Compute the projections of r onto the new basis vectors- the resulting scalars are our new coordinates for r_v.  \n",
    "3. You can check this by multiplying each scalar times the corresponding basis vector and add the two resulting vectors together- which should equal the original r_u in subspace u.\n",
    "\n",
    "$$\\frac{r_u \\cdot v_1}{|v_1|^2}$$\n",
    "\n",
    "$$\\frac{r_u \\cdot v_2}{|v_2|^2}$$\n",
    "\n",
    "#### Linearly Independent\n",
    "Two ways to check for linear independence. \n",
    "(1)If the determinant $\\neq 0$ then the vectors are linearly independent.  \n",
    "(2) Create a system of equations set equal to zero and if the system can be solved where a variable in not zero then \n",
    "\n",
    "#### Types of Matrix Transformation\n",
    "**Reflections**\n",
    "$$\\bigg[ \\matrix{0&1\\cr 1&0}\\bigg]$$\n",
    "Flips the matrix across a diagonal line. There are many permutations to this matrix including swap 0 and 1s and adding negatives which will rotate the matrix across some respective axis. \n",
    "\n",
    "**Shear**\n",
    "$$\\bigg[ \\matrix{0&1\\cr 1&1}\\bigg]$$\n",
    "Stretches object out to the upper right corner. \n",
    "\n",
    "**Rotation**\n",
    "$$\\bigg[ \\matrix{\\cos{\\theta}&\\sin{\\theta}\\cr -\\sin{\\theta}&\\cos{\\theta}}\\bigg]$$\n",
    "The above matrix is a general expression for rotating in a 2D plane. \n",
    "\n",
    "#### Transformation Matrix\n",
    "To transform a vector between two subspaces we use a transformation matrix. Say we have 2 subspaces, i and j. To transform our vector in i to j we would make a transformation matrix, where the first column is the basis in subspace i and the second column are the basis in subspace j. We can mulitply this transformation matrix by the coordinates of the vector to find the new coordinates in the new space. If we are doing the opposite translation we can use the inverse of our transformation matrix. We can do more advanced transformations in linear algebra. For example imagine knowing the translation of a vector in subspace i and wanting to see that same translation in subspace j. We could take Our vetor in j, v_ multiply by the transformation matrix to v_i, then make the transformation in i, theen use the inverse transformation to go to v_j. There is a common pattern $A^{-1}MA$ which is a transformaiton in from a subspace, a rotation or translation, and a transformation back to the original space. \n",
    "\n",
    "Remeber order matters. We can construct multiple matrices, one for shearing, one for rotating and we can multiply those matrices to get one matrix for bother operations. \n",
    "\n",
    "#### Gaussian Elimination\n",
    "Also called row reduction, gaussian elimiation is a process of subtracting rows like you would for any system of equations problem until the matrix is in row echelon form. Echelon form, also called a triangular matrix- echelon form means all the number below the diagonal are zero. After you are in row-echelon form you can use back substitution to get the correct answers for each variable, which will look like the identity matrix. \n",
    "\n",
    "Remeber to find the identity by creating ones along the diagnol with numbers above with elimination first, and using back substitution to isolate the ones. \n",
    "\n",
    "**Using Gaussian elimiation to find the inverse matrix**. The inverse matrix will give us the identity matrix that we are looking to solve for in the previous problem. However, we can also do the inverse process- we start with the identity matrix and perform the same operations on it that we perform on our other matrix A, and when A looks like the identity matrix then our identity matrix will be equal to the inverse. \n",
    "\n",
    "#### Determinant\n",
    "Given a matrix \n",
    "$$\\bigg[ \\matrix{a&0 \\cr 0&d} \\bigg]$$\n",
    "\n",
    "The determinant is how much we grow space, the area change- there is also a special case where the determinant is 0.\n",
    "\n",
    "It will scale another matrix by ad, the amount by which it scales the argument matrix is called the determinant. The determinant is denoted $|A|$. The determinant is created with QR decomposition. As we've seen earlier with linear independence if the determinant is 0 there is linear dependence, that's because the area is 0 for a line. If you cannot get row echelon form on a matrix that is because it has linear dependence and is not inverseable. \n",
    "\n",
    "#### Einstein Summation Convention\n",
    "A convention for showing indices for matrix math. \n",
    "\n",
    "$$ab_{ik} = \\sum_j a_{ij}b_{jk}$$\n",
    "\n",
    "The summation convention gives us a good way of coding up matrix math as three loops with an accumulator on j. Also in Einsteins convention the dot product is just $u_iv_i$ instead of the more verbose drawn vectors. \n",
    "\n",
    "#### Gram-Schmidt Process\n",
    "If we have linearly independent vectors that span the space we are interested in we can create a set of orthonormal basis vectors with the gram-schmidt process. \n",
    "Arbitrary basis ${u_1, u_2, u_3...}$ and the orthonormal construction basis ${v_1, v_2, v_3...}$\n",
    "\n",
    "1. Let $v_1 = u_1$\n",
    "In some cases you divide this by the magnitude of u_1 to create a normalized vector\n",
    "2. Let $v_2 = u_2 - proj_{v1} $\n",
    "Orthoganol projection of u2 onto v1\n",
    "3. Let $v_3 = u_3 - proj_{v1} - proj_{v2}$\n",
    "Set v_3 to u_3 minus the orthoganol project to the preceeding orthonomal basis\n",
    "\n",
    "We have now gone from non-orthoganol, non-unit basis set to a set of orthonormal basis vectors. These new transformation matrices are much easier to use. \n",
    "\n",
    "## EigenProblems\n",
    "#### Eigenvalues and Eigenvectors\n",
    "*Eigen* is German for characteristic, we are looking at the characteristic properties of something. We find our eigenvectors by performing a transform and looking for the vectors that are still on the same span as before, those that are are the eigenvectors- and the associated scalar- how much the vector scaled is the eigenvalue. \n",
    "\n",
    "#### Eigen Special Cases\n",
    "**Uniform Scalar**\n",
    "In a uniform scalar all vectors are eigenvectors. In 180 degree rotaiton all vectors are eigenvectors pointing in the opposite direction, with eigenvalues -1. \n",
    "\n",
    "#### Calculating Eigenvectors\n",
    "If we assume we have an eigenvector x, and we are performing a transform with transformation matrix A, we have:\n",
    "$$Ax= \\lambda x$$\n",
    "We are looking for values that scale x without changing it we can describe our eigenvector with the above equaiton. We rewrite as $(A- \\lambda I)x$ We look for the when the terms in the paraenthesis are zero. We find the determinant to see when the transformation results in a zero. Plug in the determinate and solve in terms of lambda, then plug back into the lambda equation. Another way is to say that we find the eigenvectors by solving the characteristic polynomial. \n",
    "\n",
    "To find if a value lambda is an eigenvalue subtract it from the matrix and compute the determinant. We will get a quadratic polynomial in lambda, then find the root of that epression (set to zero). \n",
    "\n",
    "#### Eigenbasis\n",
    "Changing the basis to eigenvectors. \n",
    "\n",
    "*Diagonilzation*, A diagonal matrix is one where only numbers lie on the diagonal of the matrix. These matrices are much easier to work with for many multiplications. Diagonalization is equal to finding the matrix's eigenvalues, which turn out to be precisely the entries of the diagonal matrix. The eigenvectors make up the new set of axes corresponfing to the diagonal matrix, they are eigenbasis and the values across the diagonal is the eigenvalues. \n",
    "\n",
    "If we are doing many multiplications we want to use a diagonal matrix, we can get a matrix T, to a diagnol by way of eigen analysis and changing to eigenvector basis. Then we can apply our powers to the T matrix. \n",
    "\n",
    "If you have a large set of basis vectors you can find a set of eigenvectors that span the space that you can use to create a diagonal matrix. \n",
    "\n",
    "Using eigenbasis for our transformations means computing inverse and raising the matrix to a power will be easy.  \n",
    "\n",
    "Use the eigenvectors as a basis and sandwhich it around the orginal transformation matrix with the eigenvectors on the right side and the eigenvector inverse on the left. Now you'll get the same transformaiton matrix but it will be diagnolized. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
