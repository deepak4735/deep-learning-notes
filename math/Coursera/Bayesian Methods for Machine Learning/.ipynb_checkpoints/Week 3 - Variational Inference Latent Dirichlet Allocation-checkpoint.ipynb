{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "Topic modeling uses latent dirichlet allocation (LDA) to break down a thing into topics. We can also use topic modeling to construct sentences. We can use LDA to break down the books into vector representations of the topics and then compute similarity with Euclidean distance or Cosine similarity. \n",
    "\n",
    "### Dirichlet Distribution\n",
    "$$Dir(\\theta|\\alpha) = \\frac{1}{\\beta(\\alpha)}\\prod_{k=1}^{k}\\theta_k^{\\alpha k^{-1}}$$\n",
    "\n",
    "The dirichelet distribution is determined by the vector theta whose components should add up to one and be non-negative, this is called a **simplex**. An easy way to thing of simplex, is to have a triangle with three nodes of side length 1. And theta refers the Barycentric coordinates, or centroid of the triangle. \n",
    "\n",
    "By varying alpha we can change the shape of the distribution. The dirichlet distribution has its own constuction of mean and covariance. For example the mean of theta ia the alpha i over the value of all alphas. \n",
    "\n",
    "### Latent Dirichlet Allocation \n",
    "LDA is a generatice statistical technique used in natureal language processing. In terms of topics, a 'document' is a distribution over topics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
