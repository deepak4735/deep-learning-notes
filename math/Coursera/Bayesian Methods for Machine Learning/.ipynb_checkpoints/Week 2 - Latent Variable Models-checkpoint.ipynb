{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models and Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Variable Models\n",
    "Expectation Maximization is used to train latent variable models. Latent variable is just a hidden variable. Say we have a standard regression model where we have data and some output decision. However, in this case we have missing data- and maybe we want to determine uncertainty in our options. In this case we can use a latent variable model. \n",
    "\n",
    "There is a problem that can arise when all of our data is correlated with all of our other data- we have a model with the most amount of flexibility and least amount of structre we can imagine. We would have to assign a probability to each combination of parameters, we would also normalize with the probability of each of our parameters. \n",
    "\n",
    "In Latent variable models we deal with this by declaring a hidden variable that has direct connections to each of our parameters. Rather than computing $$p(x_1, x_2, x_3, x_4, x_5) = \\sum_{I=1}^{100}p(x_1, x_2, x_3, x_4, x_5 | I)p(I)$$\n",
    "\n",
    "We have:\n",
    "$$\\sum_{I=1}^{100}p(x_1|I)...p(x_5|I)(I)$$\n",
    "\n",
    "Where we have a conditional probability between each parameter and some universal latent, and created by us parameter. \n",
    "\n",
    "Latent variable models have less edges and are simpler but can be harder to train because of the creation of the latent variable. \n",
    "\n",
    "### Probabilistic Clustering \n",
    "Hard clustering where we assign each data point a color and use those colors to clusers. This is the most common approach. We can also do soft clusering- $p(cluster\\ idx|x)$ instead of cluster idx= $f(x)$. Soft clusering leads to each data point belonging to every cluster but with different probabilities. We might do this to handle massing data, or to tune different hyperparameters. \n",
    "\n",
    "### Gaussian Mixture Model\n",
    "A Gaussian mixture model models the clusters of data points with several gaussian distributions. We define the data with a set of Gaussian parameters. The equivalent to the index for the other types of clustering methods are using the data from a point ot look up what distribution it falls under. The GMM is much more flexible, but it has more parameters to carry. \n",
    "\n",
    "### Mixture Density Networks\n",
    "Neural networks are very good at approximating one-to-one or many-to-one relationships. MDNs will predict a class of probability distributions called MDNS- where the output value is modelled as a sum of many Gaussian random values, each with different menas and standard deviations. \n",
    "\n",
    "MDNs are great for modelling data that has multiple states or is inherently a random variable that cannot be predicted. \n",
    "\n",
    "The beauty of MDNs are their ability to model several ooutput distributions. They model data with the assumption that each datapoint has some probability to be associated to a certain Gaussian distribution. Mentioned in Bishop's paper- in machine learning we are often trying to solve inverwe problems, and are interested in learning about some state that get us the correct answer. For example- given a desired location for a robot arm- figure out all the angles the arm must rotate to. \n",
    "\n",
    "### Jensen's Inequality\n",
    "$\\mathbb{E}[g(x)] \\geq g[\\mathbb{E}(x)]$ if g is convex. \n",
    "\n",
    "### KL Divergence\n",
    "$$KL(q||p)=\\int q(x)log\\frac{q(x)}{p(x)}dx$$\n",
    "\n",
    "The KL divergence is non-symmetric, and the kl divergence between some distribution and itself is zero. The kl divergence is always non-negative.  \n",
    "\n",
    "## Expectation Maximization \n",
    "Used to train latent variable models. The general form is \n",
    "$$p(x_i | \\theta) = \\sum_{c=1}^{epsilon}p(x_i|t_i = c, \\theta)p(t_i = c| \\theta)$$\n",
    "\n",
    "First we take the log of our likelihood. Then we assume our data is made of n objects independent of the parameters. Instead of taking the product of our likelihoods we take the sum of our log likelihoods. We create a lower bound with the Jensen inequality. A lot of the details are then stepped over.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
