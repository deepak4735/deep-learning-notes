{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search\n",
    "The Monte Carlo Tree Search was created in 2006. It is a game search mehtod that overcomes the limitations of earlier game ai methods like minimax\n",
    "\n",
    "Pros:\n",
    "1. Varibale runtime- MCTS can run for however long you want it to. It is essentially a loop running simulations, so you can dictate exactly how long it runs. \n",
    "2. Reusable tree- The nodes in the generated tree typically cover all of your opponents possible moves, and can all be valid tree roots. \n",
    "3. MCTS does not dictate the evaluation function like minimax, it only needs to know if the game is over, and if so- who won. In situations where there are no good heuristic evaluation functions- monte carlo thrives. \n",
    "\n",
    "Cons:\n",
    "1. Difficult to understand conceptually\n",
    "\n",
    "#### How MCTS works\n",
    "1. Select a node\n",
    "2. Expand the tree there\n",
    "3. Simulate making a decision at that state\n",
    "4. Use what you learned from that simulation to make a node selection. \n",
    " \n",
    "In a reinforcement learning algorithm we have a policy, we follow that policy until we run into a situation where we do not know what to do. From that unknown state we expand, but we imagine there are many^many states so instead we simulate a few example state-action pairs. Each one of the edges has some action that is associated with it. We will use our rollup policy to play out each of our simulations, and then get our new Q score. We then send that new Q-function value back up the tree until we get to the root. We may want to update the older policies as we go- and our older policies are stronger as we explore more of the tree. Over time we get more accurate rollouts. \n",
    "\n",
    "In Monte Carlo tree search there is often a lot of work that goes into being clever about the expansion, keeping the right kind of statistics, and more recently having a better policy than behaving randomly for exploring new areas. \n",
    "\n",
    "It's essentially Monte Carlo policy evaluation. \n",
    "\n",
    "### Bayesian Neural Networks\n",
    "Bayesian Neural networks, rather than having weights- have distributions on the weights. We treat the weights, w as a latent variable and during prediction we marginalize w out. During training we set $$\\int p(y|x, w)p(w|Y_{train}, X_{train})dw$$ The first term for p(y) is our normal NN output. To get the expected value of the distribution we can use gibb sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying probabilistic models to data usually involves integrating a complex, multi-dimensional probability distribution. For example, calculating the expectation/mean of a model distribution involves such an integration\n",
    "**get an example of these large probability distributions**\n",
    "approximate complex integrals using stochastic sampling routines\n",
    " Markov chains are useful in that if they are constructed properly, and allowed to run for a long time, the states that a chain will take also sample from a target probability distribution. Therefore we can construct Markov chains to sample from the distribution whose integral we would like to approximate, then use Monte Carlo integration to perform the approximation.\n",
    "https://stats.stackexchange.com/questions/10213/can-someone-explain-gibbs-sampling-in-very-simple-words\n",
    "You see, in general, MCMC samplers are only asymptotically guaranteed to generate samples from a distribution with the specified conditional probabilities. But in many cases, MCMC samplers are the only practical solution available.\n",
    "\n",
    "Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. For example, we might use MCMC in a setting where we spent 20 years collecting a small but expensive data set, where we are confident that our model is appropriate.\n",
    "\n",
    "\n",
    "### Monte Carlo Markov Chain\n",
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Log Likelihood\n",
    "In Bayesian statistics we want to find the parameters that define a probability distribution that describes our data. We set our equation to the joint probability of the our data points because the chance that all of those data are selected from the same distribution is the joint probability of each marginal. We generate the equation where our joint probability is equal to the product of the Gaussian's where each gaussian has an x set to one of the points. We then solve this function with some differentiation, an we have our MLE. In practive we do not find the product of our probabilites but rather find the sum of the log gaussians. We can then take the partial derivative with respect to Mu and then solve for M to get our estimate of the probability mean. In real life the MLE is often intractable because it is too large an equation and we instead use iterative methods like the Expectation Maximizaiton. \n",
    "It's called maximum likelihood because it is about the likelihood of the parameters given the data that we have seen. \n",
    "\n",
    "The MLE is related to the least sqqures minimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
