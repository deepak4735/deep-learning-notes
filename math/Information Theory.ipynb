{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Defines fundamental limits on how much information can be transmitted between the different components. Information theory defines unbreachable limits on precisely how much information can be communicated between any two components of any system. These are the laws of information: \n",
    "1. There is a definite upper limit, the channel capacity, to the amount of information that can be communicatd through the channel\n",
    "2. This limit shrinks as the amount of noise in the channel increases\n",
    "3. This limit can very nearly be reached by judicious packaging, or encoding of data. \n",
    "\n",
    "Information is measured in bits, one bit of information allows you to choose between two equally probable alternatives. \n",
    "\n",
    "Thus log_2 is an encoding for the number of forks we woud need to encode all of the possibilities of a message. \n",
    "\n",
    "**Shannon entropy** is $$log(1/p(x))$$ \n",
    "\n",
    "The average surprise of a variable x is defined by its probability distribution p(x), and is called the entropy of p(x), represented as H(x). \n",
    "\n",
    "For unbiased probabilities we can compute the information of the p(n) and p(1-n) outcome. We can compute the average surprise or **average entropy as a the sum of the shannon information times the proportion of that information for  every possible outcome**. The sum of p(x) log(1/p(x)). This is the formula for our entropy or the average surprise. \n",
    "\n",
    "Each value of the variable x can be represented with an avergage of just over H(x) binary digits. If the values of consecutive values of random varibles are not independent then each value is more predictable, and therefore less surprising, which reudce the information carrying capability, and this is why it is important to specify whehter or not consecutive variable values are independent. \n",
    "\n",
    "If H(x) = 0,469 bits then the variable x could be used to represent m = 2^0.469 or 1.38 equiprobable values. \n",
    "\n",
    "Entropy is a measure of uncertainty, when our uncertainty is reduced we gain information. Entropy is average informaiton but what word we use depends on the context, we can say we are gien some amount of information and is equicalent to having exactly the same amount of entropy taken away. \n",
    "\n",
    "We cannot assign entropy to infinite terms because each value in a continuous variable has infinite precision and can convey infinite value. In order to assign different values to different variables all infinite terms are simply ignored. \n",
    "\n",
    "$$H(x_c) = \\int p(x_c)log(\\frac{1}{p(x_c)}dx_c$$\n",
    "\n",
    "This is **differential entropy** which we can use to compute the entropy for continuous terms. \n",
    "\n",
    "### Maximum Entropy \n",
    "A distribution of values that has as much entropy (information) as theoretically possible is a maximum entropy distribution. Max entropy distributions transmit as much information as possible, and are important because if we wish to use a variable to transmit as much information as possible then we had better make sure it has maximum entropy. There are several max ent distributions, listed below in order of decreasing numbers of constraints. \n",
    "\n",
    "1. The Gaussian Distribution\n",
    "If a variable x has a fixed variance, but is otherwise unconstrained, then the max ent distribution is the Gaussian distribution. No other distribution can provide as much information at a lower energy cost per bit. \n",
    "\n",
    "2. The Exponential Distribution\n",
    "If a variable has no values below zeor, and has a fixed mean, but it otherwise unconstrianed then its max ent is exponential. \n",
    "\n",
    "3. Uniform distribution\n",
    "If a variable has a fixed lower bound x_min and upper bound x_max but is otherwise unconstrained then the maximum entropy distribution is uniform. \n",
    "\n",
    "### Channel Capacity\n",
    "An additive channel is one where an encoded value passes through and noise (eta) is added. The channel capacity C is the maximum amount of information that a channel can provide as itr output. The rate at which the information can be transmitted depends on the entropies of three variables:\n",
    "1. Entropy of the input\n",
    "2. Entropy of the output\n",
    "3. Entropy of the noise in the channel. \n",
    "\n",
    "If the output entropy is high there is a large potential for informaiton transmission but the extent to which this is realized depends on the input entropy and the level of noise. If the noise is low then the output entorpy can be close to the channel capacity. If the noise is low then the output entropy can be close to the channel capacity, however the channel capacity gets progressively smaller as the noise  increases. Capacity is usually expressed in bits per usage, or bits per second. \n",
    "\n",
    "#### Noiseless Channel\n",
    "*Shannon's fundamental theorem for a discrete noiseless channel, first fundamental coding theorem*, we can trasmit over a noiseless channel at the rate C/H where Cis the channel capacity and H is the channel entropy. \n",
    "\n",
    "Uncertainty about the output value given an input value is the conditional entorpy H(y|x). This conditional entropy is equal to the channel noise. \n",
    "\n",
    "The average amount of information gained about the output when an input value is recieved is the same as the average amount of information gained about the input when an output value is received. This is because they have the same mutual informaiton. \n",
    "\n",
    "#### Noisy channel\n",
    "The effectrs of noise can be reduced by using error correcting codes. Any method which reduces the effects of noise also reduces the rate at which informaiton can be communicated. We might logically conclude that the only way to have zero error is to reduce communication to zero, but shannone proved that information can be communicated, with vanishingly smalbl error, at a rate which is limited only by the channel capacity. \n",
    "\n",
    "*Shannon's fundamental theorem for a discrete with noise channel, second fundamental coding theore*\n",
    "\n",
    "If H < C, there exists a coding system such that the output of the source can be transmitted over the channel with an arbitarily small frequency. \n",
    "\n",
    "1. Gaussian Channel- Noise values are drawn independently from a Gaussian Distribution. \n",
    "\n",
    "\n",
    "#### Fourier Analysis\n",
    "Allows any signal to be represented as a weighted sum of sine and cosine functions. Consider a signal x, with a value x_t at time t. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
