{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra - Vectors and Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Representation of Lines\n",
    "In two dimensions we can represent a line with the equation y = mx + b but in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Combinations and Span\n",
    "$$C_1V_1 + C_2V_2 + C_3V_3 ... C_nV_n in \\ R$$\n",
    "A linear combination is the sum of sequences of vectors parameterized by some constant c. A span of A is all the vectors you can get from a linear combination of just A. \n",
    "\n",
    "The two most common vectors are $\\hat{i}$ and $\\hat{j}$. Orthogonal unit vectors that  can represent any vector in R2. Called the basis. \n",
    "\n",
    "The **span** is the set of all possible scaled-up vectors from some set of vectors. It is the space of all the vectors that can be represented by a linear combination of vectors. \n",
    "\n",
    "You can use the linear combiantion to define right combination of constants, a and b to create a vector to any given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Independence\n",
    "Linearly dependent set- One of the vectors in the set can be represetned by some combination of the other vectors in the set. The vector than can be represeted by the others is not bringing any new directionality to the set of vectors. In the exmaple, the two vectors are colinear so the span of the two vectors is a single vector. \n",
    "\n",
    "We can use the span to tell when a vector in a set is linearly dependent/independent. \n",
    "\n",
    "A set of vectors is linearly dependant if there is a linear combination of my vectors that equal the zero vector for some c_i's where not all are 0. Atleast one is non-zero. **iff** in the notation is for 'if and only if'. \n",
    "\n",
    "We can test whether something is  linearly independent  using this rule. Break the vectors into a system of equations and solve for zero. If the only possibility is that the two equations are zero then this is a linearly independent set of vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span and linear indepedence\n",
    "If you have exactly three vectors and they span R3 then they have to be linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Subspaces\n",
    "Imagine the subspace of $R^n$, V.\n",
    "For V to be a subspace $R^n$ V contains the 0 vector, \n",
    "If there is some vector X in V then when I multiply X times any scalar is also in V. This term is called **closure under scalar multiplication**, If i take some member of my set and multiply it by a scalar it should still be in my set. If I ended up in some other vector then it isn't in my subspace. \n",
    "\n",
    "**Closure under addition**: If vector and A and vector B are in the subset V, then A+B should be in subset V. When you add two vectors in your set you should end up with a vector in your subset. \n",
    "\n",
    "If some subset V:\n",
    "1. Contains the zero vector\n",
    "2. Is closed under scalar multiplicatoin\n",
    "3. Is closed under addition\n",
    "\n",
    "Then you have a subspace, this is a definition of a subspace. \n",
    "\n",
    "If we have a set {0} then this is a subspace. \n",
    "\n",
    "If we have set $S = {\\matrix{x_1 \\cr x_2} \\in R^2 | x_1 \\geq 0}$ is not a subspace because it is not closed under scalar multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis of Subspace\n",
    "\n",
    "V = span(V_1, V_2) for which the set  vectors is linearly independent. If this is true, if the set of vectors spans the subspace and are linearly independent, the set of vectors s is a basis for V. \n",
    "\n",
    "The standard basis is (1, 0), (0, 1). You can use a basis to represent any vector in your subspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Operations\n",
    "\n",
    "**Dot Product**:\n",
    "$$\\vec{v}.\\vec{w} = v_1w_1 + v_2w_2 + v_3w_3 + v_nw_n$$\n",
    "\n",
    "$$\\vec{a}\\cdot \\vec{b} = ||a|| \\ ||b|| cos\\theta$$\n",
    "\n",
    "The dot product has to be denoted with a dot, an x is a differnt multiplication process. The dot product follows the commutative and distributive properties. The dot product, unlike the cross product can exist in any dimension. \n",
    "\n",
    "**Vector Length**:\n",
    "$||a|| = \\sqrt{a_1^2 + a_2^2 + a_3^2}$\n",
    "\n",
    "The vector length is equal to the sqaure root of  the dot product with itself. \n",
    "\n",
    "**Cross Product**\n",
    "The cross product exists only in R3. The cross product is defined as a vector that is perpendicular to both a and b, with a direction given by the right hand rule. \n",
    "\n",
    "$$\\vec{a} = \\matrix{a_1 \\cr a_2 \\cr a_3} \\ \\  \\vec{b}=\\matrix{b_1 \\cr b_2 \\cr b_3}$$\n",
    "\n",
    "$$\\vec{a}x\\vec{b}= \\matrix{a_2b_3 - a_3b_2\\cr a_3b_1-a_1b_3\\cr a_1b_2-a_2b_1}$$\n",
    "\n",
    "Each row is constructed by cross multiplying the other rows in the 3 dimensional matrix. \n",
    "\n",
    "**The dot product of vectors a and b is equal to the length of vector a X the length of vector b X cosine of the angle between them.**  \n",
    "\n",
    "**The cross product of two vectors is the length of vector A X the length of vector B X the sine of the angle between them.**\n",
    "\n",
    "$$||\\vec{a} x \\vec{b}|| = ||\\vec{a}|| \\ ||\\vec{b}|| \\ sin\\theta$$\n",
    "\n",
    "If the dot prouct is zero the two vectors are orthoganol. The dot product is maximized when the vectors are colinear. The cross product is maximized when the two vectors are orthoganol. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Vector from plane \n",
    "The **normal vector** is the vector perpendicular to the place. \n",
    "\n",
    "The dot product on a position vector on the plane and the normal vecotr will always be zero because $\\cos90 = 0$  \n",
    "\n",
    "Given the equation for postional vector on the plane we can define the normal vector by simply taking the coefficeints from the poisitonal vector and using them to dfine the normal vector in terms of $(\\hat{i}, \\hat{j}, \\hat{k})$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Transformations\n",
    "\n",
    "### Transformation with respect to a basis\n",
    "Imagine some vector x, and a transformation T. A is the transformation matrix for T with respect to the standard basis. \n",
    "$$T(\\vec{x})_B= D \\vec{x}_B$$\n",
    "D is the transformation matrix for T with respect to the basis B.\n",
    "\n",
    "Given a basis we can compute a change of basis matrix where the vectors V of the Basis, B are the column vectors of the change of basis matrix C. \n",
    "\n",
    "We can transform the matrix with either A or D but the transforms will be to different coordinate systems. \n",
    "\n",
    "$$\\bigg[\\matrix{ a& b \\cr c &d}\\bigg]$$\n",
    "\n",
    "**Determinant:**\n",
    "$$\\frac{1}{ab - cd}$$\n",
    "\n",
    "### Changing coordinate systems to help find a transformation matrix\n",
    "\n",
    "Defining a 'reflection' transformation from R2 to R2. \n",
    "\n",
    "### Orthogonal Complements\n",
    "Imagine a set of vectors with length 1, who are orthoganol to each other. These are an orthonormal set. They make for good coordinate systems where you can figure out the coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Everything\n",
    "This section is about **eigen decomposition** \n",
    "\n",
    "**matrix diagonalization** Is a seperate but related issue I do not fully understand yet. \n",
    "\n",
    "Note that if there are exactly n distinct eigenvalues in an n√ón matrix then this matrix is diagonalizable.\n",
    "\n",
    "In our transformation over matrices we have vectors that don't change when we apply our transformation, vectors who are only scaled and whose direction does not change. We like these vectors that just scaled up by a transformaiton becuase they make good basis vectors. These are natural coodinate systems. \n",
    "\n",
    "Any vector that is only scaled up by a transformaiton is called a eigenvector and the scalar that is associated with its scaling is called a eigenvalue. \n",
    "$$T(\\vec{v})=A\\vec{v}=\\lambda \\vec{v}$$\n",
    "\n",
    "When we're looking for eigenvalues we are looking for non-zero vectors. \n",
    "\n",
    "If your determinant is zero, you are not invertable, also your have linearly dependent columns.\n",
    "\n",
    "If you have a non-trivial null space it cant be invertable and its determinant has to be equal to zero. \n",
    "\n",
    "If $A\\vec{v} = \\lambda \\vec{v}$ for non-zero $\\vec{v}$ if and only if $det(\\lambda I_n - A) = 0$\n",
    "\n",
    "Lambda is a eigenvalue of A iff the $determinant(\\lambda I_n-A)= 0$\n",
    "\n",
    "$$A = \\bigg[ \\matrix{1&2 \\cr 3&4} \\bigg]$$\n",
    "\n",
    "We can use the above equation and matrix A to solve for our eigenvalues, assuming non-zero eigenvectors. \n",
    "\n",
    "## Eigenvectors and Eigenspaces\n",
    "Only non-invertable matrices have a non-trivial nullspace. \n",
    "Once you solve for lambda, the eigenvalue in the above section you can solve for your eigenspace. \n",
    "\n",
    "## Linear Transformations \n",
    "If we track a vector as a linear combination of i-hat and j-hat then when we transform our vector space- not sure yet how that looks- we can use the new coordinates for i and j to determine the the transformed vector by plugging the new i and j back in the linear combinaiton with the scalars from before. Given a record of i-hat and j-hat landing- and we can figure out where our vector lands. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
