{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LImits Review\n",
    "\n",
    "Given a function that is not defined at some point. We look at the function f(x) as x gets closer to the not defined point as c. We can say that the lim f(x) as x -> c = L if point L is the value at the undefined point C. This is not a mathematically rigourous proof but we will introduce that. \n",
    "\n",
    "Look at a range around the undefined point C, and show that the corresponding point on f(x) is in the range around L. In order to show that the limit = L you would have to do this for any range around the point C and L. Then the limit is true.\n",
    "\n",
    "## Epsilon-Delta definition of limits\n",
    "\"You can get f(x) as 'close as you want' to L by making x sufficiently close to c\". Make 'close as you want' = epsilon. I will find another number, delta given where if x is within delta of C, then f(x) will be within epsiolon of L. Here we define our rnage around C and L for denining a limit. \n",
    "\n",
    "Given $\\epsilon > 0$ we can find a $\\delta > 0$ such that the $|x-c| < \\delta \\Rightarrow |f(x) -L| < \\epsilon\n",
    "\n",
    "We can use this definition to define a limit, and say that it exists. \n",
    "\n",
    "For any positive epsilon, make delta = epsilon/2 then |f(x) -L| < epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuity \n",
    "\n",
    "(1) A jump continuity, some function jumps in value with the the function looking like two seperate funcitons. (2) A removable discontinuity is when the function jumps at some value and is not continuous. \n",
    "\n",
    "We can approach the limit of a function from both directions to test if the function is continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "$$f(x) = h(g(x))$$\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{dh}{dg} * \\frac{dg}{dx}$$\n",
    "\n",
    "Composite function derivative* inner function derivative\n",
    "\n",
    "### Power Rule\n",
    "$$\\frac{d(x^n)}{dx} = nx^{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "The slope is an interpretation of a derivative not the definition. In multiple variable calculus the slope is not a good descriptor of the kinds of functions you will see. \n",
    "\n",
    "The derivatice is the rate of change at a point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Notation\n",
    "####  Linearity \n",
    "$$d(u+v) = du + dv$$\n",
    "\n",
    "$$d(Cu) = C du$$\n",
    "#### Product Rule \n",
    "$$d(u*v)=udv + vdu$$\n",
    "#### Reciprocal Rule\n",
    "$$\\frac{d}{dx}\\bigg(\\frac{1}{v} \\bigg) = -\\frac{1}{v^2}\\frac{dv}{dx}$$\n",
    "#### Quotient Rule\n",
    "$$\\frac{d}{dx}\\bigg(\\frac{u}{v} \\bigg) = \\frac{v\\frac{du}{dx}-u\\frac{dv}{dx}}{v^2}$$\n",
    "#### Inverse Rule\n",
    "$$\\frac{d}{dx}(u^{-1}) = \\frac{1}{\\frac{du}{dx}}$$\n",
    "#### Chain Rule\n",
    "$$d(u \\circ v) = du * dv$$\n",
    "$$d(f(gx)) = f'(g(x))*g'(x)$$\n",
    "#### Natural Log Derivative\n",
    "$$\\frac{d}{dx} ln(x) = \\frac{1}{x}$$ \n",
    "#### Log Derivative \n",
    "$$\\frac{d}{dx}log_b(x) = \\frac{1}{(xln(b))}$$\n",
    "#### Exponential Differentiation\n",
    "$$\\frac{d}{dx}e^{f(x)} = e^{f(x)} * f'(x)$$\n",
    "$$\\frac{d}{dx}a^{f(x)} = a^{f(x)}*f'(x)*ln(a)$$\n",
    "\n",
    "### Trigonometric Derivatives\n",
    "$$sin'(x) = cos'(x) \\qquad cos'(x) = -sin(x)$$\n",
    "$$tan'(x) = sec^2(x) \\qquad cot'(x) = -csc^2(x)$$\n",
    "$$sec'(x) = sec(x)tan(x) \\qquad csc'(x)= -csc(x)cot(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Derivatives\n",
    "Take the partial derivatives by holding all other terms constant and taking the derivative of just the term we are concerned with, we signify these partial derivatives with $\\partial$, as in $\\frac{\\partial f}{\\partial x}$.\n",
    "\n",
    "#### Total Differentiation\n",
    "\n",
    "Differentiate in terms of t, where \n",
    "$$x= t-1;\\ y= t^2;\\ z=\\frac{1}{t}$$\n",
    "\n",
    "$$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt}+ \\frac{\\partial f}{\\partial y}\\frac{dx}{dt}+ \\frac{\\partial f}{\\partial z}\\frac{dz}{dt}$$\n",
    "\n",
    "\n",
    "\n",
    "#### The Jacobian\n",
    "The Jacobian can be used in many problems but it comes up in machine learning and optimization often as a Jacobian of a single function with many variables. If you have a function with many variables then the Jacobian is simply a vector where each entry is the partial derivative of f with respect to each one of those variables- by convention we write this as a row vector. \n",
    "$$f(x_1, x_2, x_3,...)$$\n",
    "\n",
    "$$J=\\bigg[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3} \\bigg]$$\n",
    "\n",
    "Once we compute our Jacobian we can input values for coordinates and output the maginitude of our vectors at those coordinates. We can use these vectors to construct a vector field. \n",
    "\n",
    "We can stack several Jacobians to create a Jacobian Matrix. \n",
    "\n",
    "#### The Hessian \n",
    "The Hessian can be seen as an extension of the Jacobian except where the Jacobian is composed of the first order derivatives of the function the Hessian is composed of the second-order derivatives. We can find the Hessian by finding the Jacobian first and then finding the Hessian. If our Jacobian is a vector of lengther 3, our Hessian will be a 3x3 matrix as we have to take the second order derivative of the first keeping all other variables constant. \n",
    "\n",
    "#### Multivariate Chain Rule\n",
    "Given a function of n variables, we can write it as f(**x**) where the **x** is in bold to signify a n-dimensional vector. Each of the components of our x vector are each of themselves a function of some variable t. Here $\\frac{\\partial f}{\\partial x}$ and $\\frac{dx}{dt}$ are vectors \n",
    "\n",
    "$$f({\\bf x}) = f(x_1, x_2, x_3, ..., x_n)$$\n",
    "\n",
    "$$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x}* \\frac{dx}{dt}$$\n",
    "  \n",
    "#### Taylor/Power Series\n",
    "$$f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(p)}{n!}(x-p)^n$$\n",
    "\n",
    "$$f(x)= f(p) + \\frac{df}{dx}(p)\\frac{(x-p)^n}{n!}+...$$\n",
    "\n",
    "Power series are named so because they are coefficients in front of increasing powers of x, where each term that we add increases our approximation- there is often a pattern in the coefficients. Many times we use only a few terms in the taylor series hopeing that that is a good enough approximation. We call these the zeroth, first, second, and third order approximations- these are called the truncated series. \n",
    "\n",
    "The Taylor series tells us that if we know everything about some function at a point then we can reconstruct the function everywhere else. This is only true for functions that are *well-behaved*, functions that are continuous and that you can differentiate as many times as you want. \n",
    "\n",
    "#### Maclaurin Series\n",
    "$$g(x) = \\sum_{n=0}^{\\infty} \\frac{1}{n!}f^{(n)}(0)x^n$$\n",
    "\n",
    "The Maclaurin series is the Taylor series where we are looking at the point x=0. It assumes that if we know everything about a function at point 0 we can reconstruct it. The Taylor series is a more general case where we can look at an point. \n",
    "\n",
    "We find the Maclaurin series with the above formula where we comput e the nth derivative of our function at f(0) and divide it by the nth factorial to find our coefficient- which we then multiply it by the respective nth x and add it to our other terms. \n",
    "\n",
    "#### Linearisation\n",
    "$$f(x + \\Delta x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x)}{n!}\\Delta x^n$$\n",
    "\n",
    "We have changed the notation of x-p to delta-p to signify a short distance away from p. We can then drop the P. Giving us\n",
    "\n",
    "$$g_1(x + \\Delta X) = f(x) + f'(x)(\\Delta x)$$\n",
    "\n",
    "We can use this in combination with an order term O, as an error term in our equation. We can rework the equation so that we can find the error. \n",
    "\n",
    "#### Multivariate Taylor Series\n",
    "A combination of the Jacobian and the Taylor Series to construct function approximations for hypersurfaces. \n",
    "\n",
    "#### Newton - Raphson\n",
    "$$\\partial x= -f(x)/f'(x)$$ \n",
    "\n",
    "Consider we have a distribution, and we want to fit an equation to the distribution so we don't have to carry around the data points and can instead use the equation with two parameters, a mean and a width. The Newton-Raphson method is a method for fitting an equation to that data. Dealing with multi-dimensional data makes analytical methods and lots of plotting very expensive. Newton-Raphson is an *iterative method* that is much more computationally tractable. \n",
    "\n",
    "#### Lagrange Multiplier\n",
    "Lagrange multipliers are a technique for finding the minimum of a function subject to a constraint. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
