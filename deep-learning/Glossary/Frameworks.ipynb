{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "\n",
    "1. torch.autograd has the method .Variable() because in the .variable Backward() will autmoatically compute the gradients for whatever it wraps. torch.autograd.Variable records the history of operations on a tensor. .backward() computes the gradients. These gradients are collected in a Variable class' grad member. Backward() computes the gradinets of that tensor with respect to the leaves of the computation graph - all the inputs that influenced that value\n",
    "\n",
    "\n",
    "1. Writing your own layers/modules:\n",
    "class AddNoise(torch.nn.Module):\n",
    "    def __init__(self, mean=0.0, stddev=0.1):\n",
    "        super(AddNoise, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, input):\n",
    "        noise = input.clone().normal_(self.mean, self.stddev)\n",
    "        return input + noise\n",
    "        \n",
    "\n",
    "2. Using sequential container for having multiple layers\n",
    "import torch\n",
    "In [2]: from torch import nn\n",
    "In [3]: from torch.autograd import Variable\n",
    "In [4]: model = nn.Sequential(\n",
    "   ...:     nn.Conv2d(1, 20, 5),\n",
    "   ...:     nn.ReLU(),\n",
    "   ...:     nn.Conv2d(20, 64, 5),\n",
    "   ...:     nn.ReLU())\n",
    "   ...:\n",
    "\n",
    "In [5]: image = Variable(torch.rand(1, 1, 32, 32))\n",
    "In [6]: model(image)\n",
    "Out[6]:\n",
    "\n",
    "3. Loss functions\n",
    "Loss functions are part of something called criterions that are parameterized upon construction and then used as plain functions, loss functions are inside of the torch.nn module\n",
    "\n",
    "4. Optimizers\n",
    "The torch.optim package provides a series of optimizers. \n",
    "\n",
    "5. Data Loading\n",
    "The helper classes and functions are found in the torch.utils.data modeule. \n",
    "\n",
    "Two parts are Dataset, which encapsultes the source of data, and Dataloader, which is responsible for loading a datawt, possibly in parallel. \n",
    "\n",
    "New datasets are created by subclassing torch.utils.data.Dataset\n",
    "import math\n",
    "\n",
    "class RangeDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, start, end, step=1):\n",
    "    self.start = start\n",
    "    self.end = end\n",
    "    self.step = step\n",
    "\n",
    "  def __len__(self, length):\n",
    "    return math.ceil((self.end - self.start) / self.step)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    value = self.start + index * self.step\n",
    "    assert value < self.end\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
