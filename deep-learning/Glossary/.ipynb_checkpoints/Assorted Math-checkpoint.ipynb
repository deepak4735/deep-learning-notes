{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Likelihood \n",
    "\n",
    "### Log Likelihood trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy\n",
    "\n",
    "Entropy is a measure of uncertainty. The more we know about something the lower the entropy. If a language model captures more of the language, then the entropy should be lower. We can use entropy as a measure of the quality of our models. \n",
    "\n",
    "Estimating probability distributions from data. \n",
    "\n",
    "Over-arching principle is that when nothing is known, the distribution should be as uniform as possible, that is, have maximal entropy. \n",
    "\n",
    "Maximum entropy estimates the conditional distribution of the class label given a document. \n",
    "\n",
    "Maximum entropy is highest uncertainty. \n",
    "\n",
    "It is a conditional, discriminative model that allows for mutually dependent variables. \n",
    "\n",
    "#### Example\n",
    "Let $X$ be a discrete random variable, it takes values that are distributed according to the probability P. \n",
    "\n",
    "A discrete random variable only includes variables from a countable set- so not a set of real or rational numbers. \n",
    "\n",
    "The entropy of $X$ is denoted by $H(x) = -\\Sigma_{x\\epsilon X}P(x)*log P(x) $\n",
    "\n",
    "$X\\epsilon\\{1,2,3\\} =: X, p=(1/2, 1/4, 1/4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matrix\n",
    "\n",
    "Measures the variance between parameters. \n",
    "\n",
    "$covariance = 1/N (\\Sigma(x-\\mu_x) * (y-\\mu_y))$\n",
    "\n",
    "The diagonal of the covariance matrix is the covariance of a parameter with itself. It is the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Information theory is important when you want to talk probability distributions. What is most important for us to talk about is uncertainty, which we call entropy. Suppose we have a coin with a 100% chance of heads, it would have an entropy of 0. \n",
    "\n",
    "### Information\n",
    "$ I(x) = -logP(x)$\n",
    "\n",
    "### Entropy\n",
    "$H(x) = E_{x~p}[I(x)]=-E_{x~p}[logP(x)]$\n",
    "\n",
    "\n",
    "### KL Divergence \n",
    "\n",
    "$D_{KL}(P||Q)=E_{x~p}[log\\frac{P(x)}{Q(x)}] = E_{x~p}[logP(x) - logQ(x)]$\n",
    "\n",
    "KL Divergence let's us quantify how different two distributions are. It is not symmetric, so if you switch the order of the arguments you get a different result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Maxwell- Boltzmann Distribution \n",
    "\n",
    "The Maxwell-Boltzmann distribution is the distribution of speeds of air particles. \n",
    "\n",
    "Temperature is proportional to average kinetic energy in the system. If we were to create a chart with speed on the x axis and # of molecules on the y axis and draw the distribution of particles we would get the maxwell-boltzmann distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
