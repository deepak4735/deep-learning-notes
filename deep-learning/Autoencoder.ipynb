{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "An autoencoder learns a lossy, compressed representation of data.\n",
    "An autoencoder has three parts:\n",
    "1. Encoding function \n",
    "2. Decoding function\n",
    "3. Distance function between the amount of information loss between the compressed representation of the data and the decompresed representation. \n",
    "\n",
    "Two practical applications of autoencoders are data denoising and dimensional reduction for data visuzalization. \n",
    "\n",
    "t-SNE is a good algorithm for visualizing low-dimensional data, one use for autoencoders is to use an autoencoder to compress data into a lower-dimensinal space, like 32 dimensional, and then use t-SNE for mapping the compressed data to a 2D plane. \n",
    "\n",
    "### Encoder and Decoder\n",
    "\n",
    "decoder:= P(X|z)\n",
    "encoder:= Q(z|X)\n",
    "\n",
    "The encoder Q is a net, outputting the mean, Mu, and the standard deviation, Sigma, of the encoded data. \n",
    "\n",
    "### Xavier Initialization\n",
    "\n",
    "For our weight initialization we want a Gaussian distribution with 0 mean and finite variance.\n",
    "\n",
    "We want to choose a finite variance that will not lead to exploding or vanishing gradients. To do this we use a variance that is equal to 1/N, where N is the number of input neurons. In the original paper by Glorot and Bengio *Understanding the difficulty of training deep feedforward neural networks* they use 1/N, where N is (N_in + N_out)/2. More recent papers simply use the number of input neurons to reduce computational complexity. \n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "In a variational autoencoder we are random sampling from the encoder's distribution. In our computation graph we cannot backpropogate through this random node because we cannot compute a gradient for our parameters in a random function. To fix this we move the learnable parameters outside our sampling function- now there are no gradient calculations needed for the stochastic node in our graph. \n",
    "\n",
    "### Unnesecary Notes\n",
    "\n",
    "Z is the latent vector \n",
    "\n",
    "### Things I want to try\n",
    "\n",
    "Changing the size of the latent vector and looking at image reconstruction\n",
    "\n",
    "Letting the user select values for the latent vector- or some system in that vein \n",
    "\n",
    "Looking at how limited the autoencoders is in terms of how many examples it needs and how much growth in number of samples it needs to approximate a greater diversity of images. \n",
    "\n",
    "Can I use an autoencoder to encode data for a GA- what happens what does that look like. I know that is what Hardmaru did, or something like it but I want to try it myself. I'm not sure exactly how to do this- but its definantly going to happen. This sounds liek it is super computaitonally expensive but I'd like to read more about the interaction of GA and machine learning.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot= True)\n",
    "\n",
    "mb_size = 64 #mini batch size\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1] #784, number of pixels 28x28\n",
    "Y_dim = mnist.train.labels.shape[1] #10, number of categories\n",
    "h_dim = 128 \n",
    "c = 0\n",
    "lr = 1e-3 #learning rate\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim]) # weights for linear layer\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim]) # weights for mean\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])# weights for variance \n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "    \n",
    "# Encoder\n",
    "# Multiple the input by weights and add bias, then multiply activation by the mu/variance weights + bias\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1)) # .repeat is for some broadcasting issue with pytorch\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1) \n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mnist.train.images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, log_var):\n",
    "    # Using reparameterization trick to sample from a gaussian\n",
    "    eps = Variable(torch.randn(mb_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "# Pass compression to through linear layer and then sigmoid \n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1)) \n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "\n",
    "for it in range(100000):\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Forward\n",
    "    z_mu, z_var = Q(X)\n",
    "    z = sample_z(z_mu, z_var)\n",
    "    X_sample = P(z)\n",
    "\n",
    "    # Loss\n",
    "    recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False)\n",
    "    kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var)\n",
    "    loss = recon_loss + kl_loss\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    solver.step()\n",
    "\n",
    "    # Housekeeping\n",
    "    for p in params:\n",
    "        p.grad.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
