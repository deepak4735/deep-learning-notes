{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "## Structured, time series, and language models\n",
    "\n",
    "### Grocery Store Market Research\n",
    "First Part of the video- goes over structured data, dont remember when it starts but it ends at about the 1:24 mark.\n",
    "\n",
    "### Language Modeling\n",
    "Starts around 1:25 mark. He downloads an archix excerpts data. Creates a language model for generating archix papers. At 1:37 language model with IMDB large movie review set, contains 50k reviews and classified by sentiment. We use this data to pre-train, and then \n",
    "\n",
    "### Notes\n",
    "1. Dropout. Pick random activations and delete them, the value P, is the probability that you delete some cell. In each minibatch we apply dropout as a way to prevent overfitting. Before dropout there were very few options to deal with overfitting. About 4 years ago Hinton and his lab came up with the idea of dropout. The default value for dropout is 0.25 for fastai framework but as you create larger models with more parameters you have to bump up the p value of dropout. In fastai you can also pass in multiple dropout levels. Jeremy still plays a lot with dropout and doesn't have any rules of thumb. Two things to try are using the same value of dropout on every layer and only using dropout on the last layer. \n",
    "2. Categorical variable. In the structured data example there is an example where the date was used as a categorical variable. This is an interesting and important choice because even though date can be a continuous variable we can image shopping to be discrete as Tuesdays may categorically look different than Wednesdays. You can make a continous variavle categorical but it is very difficult with floating oint. The number of categories in a categorical variable is called the cardinality of a category. Jeremy generally does not bin varaibles. \n",
    "3. Pandas asType. Loop through the categoies an use asType category and asType float 32 for continuous variables and join.  \n",
    "4. Normalize test and train set with the same values.\n",
    "5. The simple linear, fully connected architectures like the example problems don't have much more than linear layers, relu and a softmax layer for classification at the end. \n",
    "6. Embedding matrix. Start out with an integer, like for a category, and turn them into a float so that\n",
    "7. Neural network with categorical variable. We have a row for each category. We decide how many columns to have for each categorical variable by taking the minimum of the cardinality + 1 divides by 2 and 50. We can also use one-hot encoding- called dummy variables, to encode categorical variables. The idea of this embedding is called a distributed representation. \n",
    "8. Language modeling. Predict the next words in the sentence. Pytorch has torchText, an NLP library. \n",
    "    1. First step, tokenize words. Spacey is a great tokenizer- torch text works with the spacey tokenizer. \n",
    "    2. Create a field, pass what you want to do to text, like tokenize and lowercase. \n",
    "    3. Then create fastai model object, pass train, test, and validation test set. Create language model and pass path to the te. xt, the field object, files for training and validation, bathc size, and min_freq- commonly used in NLP this set a trehshold for words as we are about to create an index for every word and we may not want to include words that have less than a certian frquency. BPTT, back prop through time give you the length of the sentences that you feed into the network. Generally we will tokenize rather than lemmatize or stem. \n",
    "    4. BPTT. Each batch will be be batchsize wide by BPTT long, something that torchtext does is use varible length so that each group isn't exactly bptt long but approzimately bptt long. \n",
    "9. In a recent paper Stephen Merity introduces AWD LSTMs, a key feature in this model is that it provides great regulaarization through dropout, At 2:01 in the video Jeremy talks about setting the parameters for the AWD LSTM\n",
    "10. Gradient clipping, A method of adaptive gradients that 'cut the gradient if the learning rate is too large' or something. His explanation is unclear. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
