{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "## Convolutional Neural Nets\n",
    "\n",
    "1. When we look at learning rates it is presented on a logarithmic scale where 10^-1 is .1 also written as 1e^-1 or 1e^-2 for .01\n",
    "2. This is in the lesson 1 notes but agian we seee the trick for finding the correct learning rate where we plot the loss and the learning rate in log scale and we find the point before the lowest point in the chart\n",
    "3. Data augmentation is one of the most important parts of deep learning, in the fastai framework we have methods that do most of this work for us- this includes things like moving the image, reflection of an image, flipping an image, and zooming in and out. \n",
    "4. **Stochastic Gradient Descent with Restarts** As you get closer to the global minima you want to decrease you're learning rate. Decreasing your learning rate as you get closer is called annealing. Most implementations of annealing are quite hackey, like step-wise annealing. A really good curve for the change in annealing rate is half of a cosign curve- when we use this curve to dictate the change in our learning rate it is called cosign aneealing. Cosign annealing can be great but is fragile in a high dimensional space, the response to this is stochastic gradient descent with restarts. If we have a learning rate schedule with multiple cosign curves such that - the rate decreases and then spikes up to the top and then down the cosign curve again we have restarts that keep us from getting stuck at local minima. \n",
    "5. Jeremey Howard claims that usign stochastic gradient descent with restarts in combination with the **learning rate trick** allows him to very quickly find a very good learning rate. \n",
    "6. It can be useful to save and reload your layers because many actions wil create temporary variables - all recomputed activations end up here. This can lead to some errors, where maybe there are half computed layers or something. A way to deal with these error is to delete the temp folder and run again. \n",
    "7. We create an array of learning rates that correspond to different layers of our net we will retrain, using larger learning rates for later layers. This is a technique that jeremy howard introduces and calls **differential learning rates**. This is apparently a great secret for getting the most out of models. \n",
    "8. You can unfreeze and relearn any arbitrray layer and the layers aftwerward. \n",
    "9. Cycles as the word is used in the fastai library refers to cycles of the learning rate with restarts. You can vary the cycle length to allow for better exploration for our learning rate. cycle_mult= provides multiple modes the the learning rate schedule. \n",
    "10. TTA- test time augmentation, running inference with a model over a single image that has been augmented across the suite of changes that are made to evyer image in the library. \n",
    "11. There are many types of augmentation, like adding a border around the image- jeremy doesn't find that it matters. Jeremy does some reflection padding, but finds that zooming is a better technique. \n",
    "12. Data augmentation in non-image data. It doesn't really exist for non-image domains. \n",
    "13. Pytorch is more than just a deep learning library it gives us a way to write arbitrarty code for a GPU so there are many things written on top of pytorch, like Uber's Pyro- a probablistic programming framework. \n",
    "14. Early iterations jeremy will use super small size images to speed up processing. If you get a CUDA memory error- go to kernel restart and change to a smaller batch size. \n",
    "15. Another trick- after you have trained on your dataset with smaller size images you can continue to train with the same but larger sized dataset, another besnefit of pytorch. Training on smaller images and then continuing to train on larger size images is a way to get state of the art results without overfitting. \n",
    "\n",
    "\n",
    "### Easy steps to train a world class classifier\n",
    "1. Enable data augmentation, and precompute= True\n",
    "2. Use lr_find() to find the highest learning rate where loss is still clearly improving\n",
    "3. Train last layer from precomputed activations for 1-2 epochs\n",
    "4. Train last layer with data augmentation (i.e. precompute = False) for 2-3 epochs with cycle_len=1\n",
    "5. Unfreeze all layers\n",
    "6. Set earlier layers to 3x-10x lower learning rate than the next higher layer\n",
    "7. Use lr_find() again\n",
    "8. Train full network with cycle_mult=2 until over-fitting\n",
    "\n",
    "\n",
    "16. If you change the batch size massively you should recompute the learning rate but in practice all you are doing is cutting the batch size in half or a quarter so you don't have to change the learning rate by much. \n",
    "17. Another architecture to train on other than resnet34 is resnext50 (the runner up in imagenet 2017), it takes more memory and is slower but it is incredibly accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
