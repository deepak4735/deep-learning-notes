{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6\n",
    "## Interpreting Embeddings, RNNs from scratch\n",
    "\n",
    "\n",
    "### Review of Collaborative Filtering  \n",
    "1. Property. A wrapper class that is a getter method. You can call it like an attribute without the parenthesis but it is actually a getter method. \n",
    "2. Variables. If you pass a tensor into a model wrap it in a 'V' for variable. In later editions of pytorch you won't have to do this. \n",
    "3. Pytorch vs Numpy. Do as much as you can in numpy and when you can't use pytorch. Jeremy brings up an important point here, that Pytorch is not strictly a ml framework but rather a python based gpu framework that is much more robust. Thus, his advice is to do as much as you can in numpy land and then convert to pytorch. I think when you send tensors from numpy to pytorch you have to wrap in the 'V' for variable. \n",
    "4. Interpretting PCA/Embeddings. In the computational linear algrbra course they go over PCA and SVD in depth. What I really like is Jeremy printing the varous components from PCA in order and then in reverse order and categorizing by eye test the different categories for the movies. \n",
    "\n",
    "### Rossman Grocery Review\n",
    "1. Entity Embeddings/ Categorical Embeddings- I still do not understand these, but they are useful in neural nets and other machine learning methods- basically these are super dope for categorical or structured data. \n",
    "2. Plotiing PCA embedding vectors. Amazing, by plotting the first two componenets from PCA of German cities you get something that resembles Germany, from a paper Entity Embeddings for Categorical Variables. **A good tip here is to visualize embeddings with something as simple as plotting the first two embeddings on an x-y plot.** There are scriptgrams- for NLP. They had an unsupervised algorithm for word2vec and they needed to generate labels. They generate labels by creating random bad examples and generating a machine learning model that then spots the bad sentences with those labels- it has now created embeddings that represent the good sentences. This is called a dummy test. \n",
    "3. Modern NLP. Glove and Word2Vec are linear models that according to Jeremy do not create rich embeddings of words and that we need to move beyond these linear models for better NLP algorithms. He mentions the state of the art result from a previous video- could be worth looking at. \n",
    "4. Fake task relationship learning. A summary of supervised learning with dummy tasks. An interesting area of what and how much does the fake task matter. The ultimate fake task is the autoencoder- it starts with a larger set of activations and leads to a lower set of activations and it learns its features.  \n",
    "5. Don't touch your data unless you have tested the effect or changing that data. \n",
    "\n",
    "### Recurrent Neural Networks\n",
    "Begins with a implementation of a neural network for finding some metric of a line with gradient descent as a way of understanding gradient descent.\n",
    "1. Loss function with continuous variable. Mean squared error. \n",
    "2. Loss functions. You can have multiple loss functions. \n",
    "3. Activations. Is a number.\n",
    "4. BPE,  byte-pair-encoding. An encoding for rnns that uses not words, or chracter level but  a byte pair encoding that I believe is somewhere in the middle, this is talked about more in  part 2. \n",
    "5. TanH. Used in hidden state to hidden state transitions where we have input to the layer in addition to the hiden state passed from the previous layer. \n",
    "6. Concatenated input and hidden state. I think this in about the activations so far and the input to this layer in the character by character rnn, we want to concatenate (where Jeremy intitialy adds) the h ( hidden state so far) and the inp (the input char to this layer) and then pass them into the TanH functions, this shouls also help with understanding the need for the tanH in this model in addition to the relu activations. Jeremy later adds that if you have multiple things you want to concatenate them instead of add. Once you have concatenated them, you can always change them back to the fixed size by chucking them into a matrix product. \n",
    "7. Initialize RNN with identity matrix to fight exploding/diminishing gradient. RNNS Can be sensitive to initial conditions (I think this has to do with recurrent/time-dependent structure). We deal with this by initializing with the identity matrix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
